"""
Test RAG Pipeline functionality.
"""

import sys
import os
import tempfile
import shutil
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime

sys.path.append(os.path.dirname(__file__))

from models import DocumentType, ChromaDBConfig, KnowledgeFragment, Collection
from rag_pipeline import RAGPipeline
from manager import KnowledgeBaseManager


def test_rag_pipeline_initialization():
    """Test RAG pipeline initialization."""
    print("Testing RAG Pipeline Initialization...")
    print("=" * 50)
    
    # Test 1: Basic initialization
    print("\n1. Testing basic initialization:")
    
    rag_pipeline = RAGPipeline()
    
    if rag_pipeline:
        print("   âœ“ RAG pipeline initialized successfully")
        print(f"   - Enabled: {rag_pipeline.enabled}")
        print(f"   - Max context length: {rag_pipeline.max_context_length}")
        print(f"   - Min relevance score: {rag_pipeline.min_relevance_score}")
        print(f"   - Max knowledge fragments: {rag_pipeline.max_knowledge_fragments}")
    else:
        print("   âœ— RAG pipeline initialization failed")
    
    # Test 2: Initialization with components
    print("\n2. Testing initialization with components:")
    
    mock_kb_manager = Mock()
    mock_llm_service = Mock()
    mock_llm_service.return_value = "Mock LLM response"
    
    rag_pipeline_with_components = RAGPipeline(
        knowledge_base_manager=mock_kb_manager,
        llm_service=mock_llm_service
    )
    
    if rag_pipeline_with_components:
        print("   âœ“ RAG pipeline with components initialized successfully")
        print(f"   - Knowledge base manager available: {rag_pipeline_with_components.knowledge_base_manager is not None}")
        print(f"   - LLM service available: {rag_pipeline_with_components.llm_service is not None}")
    else:
        print("   âœ— RAG pipeline with components initialization failed")
    
    print("\n" + "=" * 50)
    print("âœ“ RAG pipeline initialization tests completed!")


def test_rag_pipeline_collection_management():
    """Test RAG pipeline collection management."""
    print("\nTesting RAG Pipeline Collection Management...")
    print("=" * 50)
    
    rag_pipeline = RAGPipeline()
    
    # Test 1: Set and get selected collections
    print("\n1. Testing collection selection:")
    
    test_collections = ["collection_1", "collection_2", "collection_3"]
    rag_pipeline.set_selected_collections(test_collections)
    
    selected = rag_pipeline.get_selected_collections()
    
    if selected == test_collections:
        print("   âœ“ Collection selection works correctly")
        print(f"   - Selected collections: {len(selected)}")
    else:
        print("   âœ— Collection selection failed")
        print(f"   - Expected: {test_collections}")
        print(f"   - Got: {selected}")
    
    # Test 2: Empty collection list
    print("\n2. Testing empty collection list:")
    
    rag_pipeline.set_selected_collections([])
    empty_selected = rag_pipeline.get_selected_collections()
    
    if empty_selected == []:
        print("   âœ“ Empty collection list handled correctly")
    else:
        print("   âœ— Empty collection list handling failed")
    
    # Test 3: None collection list
    print("\n3. Testing None collection list:")
    
    rag_pipeline.set_selected_collections(None)
    none_selected = rag_pipeline.get_selected_collections()
    
    if none_selected == []:
        print("   âœ“ None collection list handled correctly")
    else:
        print("   âœ— None collection list handling failed")
    
    print("\n" + "=" * 50)
    print("âœ“ Collection management tests completed!")


def test_rag_pipeline_status_and_control():
    """Test RAG pipeline status and control methods."""
    print("\nTesting RAG Pipeline Status and Control...")
    print("=" * 50)
    
    # Create mock knowledge base manager
    mock_kb_manager = Mock()
    mock_kb_manager.list_collections.return_value = [
        Mock(id="coll_1", name="Test Collection")
    ]
    mock_kb_manager.get_knowledge_base_stats.return_value = {
        "total_collections": 1,
        "total_documents": 5,
        "total_chunks": 50,
        "storage_path": "/test/path"
    }
    
    rag_pipeline = RAGPipeline(knowledge_base_manager=mock_kb_manager)
    
    # Test 1: Knowledge base status
    print("\n1. Testing knowledge base status:")
    
    status = rag_pipeline.get_knowledge_base_status()
    
    if status:
        print("   âœ“ Status retrieved successfully")
        print(f"   - Enabled: {status['enabled']}")
        print(f"   - Manager available: {status['manager_available']}")
        print(f"   - Total collections: {status['total_collections']}")
        print(f"   - Total documents: {status['total_documents']}")
        print(f"   - Can process queries: {status['can_process_queries']}")
    else:
        print("   âœ— Status retrieval failed")
    
    # Test 2: Enable/disable knowledge base
    print("\n2. Testing enable/disable functionality:")
    
    rag_pipeline.disable_knowledge_base()
    if not rag_pipeline.enabled:
        print("   âœ“ Knowledge base disabled successfully")
    else:
        print("   âœ— Knowledge base disable failed")
    
    rag_pipeline.enable_knowledge_base()
    if rag_pipeline.enabled and not rag_pipeline._fallback_mode:
        print("   âœ“ Knowledge base enabled successfully")
    else:
        print("   âœ— Knowledge base enable failed")
    
    # Test 3: Fallback mode
    print("\n3. Testing fallback mode:")
    
    rag_pipeline.enable_fallback_mode()
    if rag_pipeline._fallback_mode:
        print("   âœ“ Fallback mode enabled successfully")
    else:
        print("   âœ— Fallback mode enable failed")
    
    rag_pipeline.disable_fallback_mode()
    if not rag_pipeline._fallback_mode:
        print("   âœ“ Fallback mode disabled successfully")
    else:
        print("   âœ— Fallback mode disable failed")
    
    # Test 4: Should use knowledge base
    print("\n4. Testing should_use_knowledge_base logic:")
    
    should_use = rag_pipeline.should_use_knowledge_base()
    if isinstance(should_use, bool):
        print(f"   âœ“ Should use knowledge base: {should_use}")
    else:
        print("   âœ— should_use_knowledge_base returned invalid type")
    
    print("\n" + "=" * 50)
    print("âœ“ Status and control tests completed!")


def test_rag_pipeline_prompt_building():
    """Test RAG pipeline prompt building functionality."""
    print("\nTesting RAG Pipeline Prompt Building...")
    print("=" * 50)
    
    rag_pipeline = RAGPipeline()
    
    # Test 1: Basic prompt building
    print("\n1. Testing basic prompt building:")
    
    original_query = "What is artificial intelligence?"
    knowledge_context = """ç›¸å…³çŸ¥è¯†å†…å®¹ï¼š

[çŸ¥è¯†ç‰‡æ®µ 1]
æ¥æºï¼šai_basics.pdf
ç›¸å…³åº¦ï¼š0.95
å†…å®¹ï¼šäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ç”±æœºå™¨å±•ç¤ºçš„æ™ºèƒ½ï¼Œä¸äººç±»å±•ç¤ºçš„è‡ªç„¶æ™ºèƒ½å½¢æˆå¯¹æ¯”ã€‚

[çŸ¥è¯†ç‰‡æ®µ 2]
æ¥æºï¼šml_guide.pdf
ç›¸å…³åº¦ï¼š0.87
å†…å®¹ï¼šæœºå™¨å­¦ä¹ æ˜¯æ•°æ®åˆ†æçš„ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–åˆ†ææ¨¡å‹çš„æ„å»ºã€‚"""
    
    enhanced_prompt = rag_pipeline.build_enhanced_prompt(original_query, knowledge_context)
    
    if enhanced_prompt and len(enhanced_prompt) > len(original_query):
        print("   âœ“ Enhanced prompt built successfully")
        print(f"   - Original query length: {len(original_query)}")
        print(f"   - Enhanced prompt length: {len(enhanced_prompt)}")
        print(f"   - Contains knowledge context: {'ç›¸å…³çŸ¥è¯†å†…å®¹' in enhanced_prompt}")
        print(f"   - Contains original query: {original_query in enhanced_prompt}")
    else:
        print("   âœ— Enhanced prompt building failed")
    
    # Test 2: Long context truncation
    print("\n2. Testing long context truncation:")
    
    long_context = "A" * 5000  # Longer than max_context_length
    truncated_prompt = rag_pipeline.build_enhanced_prompt(original_query, long_context)
    
    if "[å†…å®¹å·²æˆªæ–­...]" in truncated_prompt:
        print("   âœ“ Long context truncated correctly")
        print(f"   - Truncated prompt length: {len(truncated_prompt)}")
    else:
        print("   âœ— Long context truncation failed")
    
    # Test 3: Empty context handling
    print("\n3. Testing empty context handling:")
    
    empty_context_prompt = rag_pipeline.build_enhanced_prompt(original_query, "")
    
    if empty_context_prompt:
        print("   âœ“ Empty context handled correctly")
        print(f"   - Prompt length: {len(empty_context_prompt)}")
    else:
        print("   âœ— Empty context handling failed")
    
    print("\n" + "=" * 50)
    print("âœ“ Prompt building tests completed!")


def test_rag_pipeline_knowledge_search():
    """Test RAG pipeline knowledge search functionality."""
    print("\nTesting RAG Pipeline Knowledge Search...")
    print("=" * 50)
    
    # Create mock knowledge fragments
    mock_fragments = [
        KnowledgeFragment(
            content="äººå·¥æ™ºèƒ½æ˜¯ç”±æœºå™¨å±•ç¤ºçš„æ™ºèƒ½ï¼Œä¸äººç±»çš„è‡ªç„¶æ™ºèƒ½å½¢æˆå¯¹æ¯”ã€‚",
            source_document="ai_basics.pdf",
            collection_name="AI Knowledge",
            relevance_score=0.95,
            metadata={"document_type": "knowledge", "chunk_index": 0}
        ),
        KnowledgeFragment(
            content="æœºå™¨å­¦ä¹ æ˜¯æ•°æ®åˆ†æçš„ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥è‡ªåŠ¨åŒ–åˆ†ææ¨¡å‹çš„æ„å»ºã€‚",
            source_document="ml_guide.pdf",
            collection_name="AI Knowledge",
            relevance_score=0.87,
            metadata={"document_type": "knowledge", "chunk_index": 1}
        ),
        KnowledgeFragment(
            content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ æ–¹æ³•çš„ä¸€ä¸ªæ›´å¹¿æ³›å®¶æ—çš„ä¸€éƒ¨åˆ†ã€‚",
            source_document="dl_intro.pdf",
            collection_name="AI Knowledge",
            relevance_score=0.75,
            metadata={"document_type": "knowledge", "chunk_index": 0}
        )
    ]
    
    # Create mock knowledge base manager
    mock_kb_manager = Mock()
    mock_kb_manager.search_knowledge.return_value = mock_fragments
    mock_kb_manager.list_collections.return_value = [
        Mock(id="coll_1", name="AI Knowledge")
    ]
    
    rag_pipeline = RAGPipeline(knowledge_base_manager=mock_kb_manager)
    rag_pipeline.enabled = True
    rag_pipeline.set_selected_collections(["coll_1"])
    
    # Test 1: Knowledge search preview
    print("\n1. Testing knowledge search preview:")
    
    preview_results = rag_pipeline.search_knowledge_preview(
        query="What is artificial intelligence?",
        collections=["coll_1"],
        top_k=3
    )
    
    if preview_results:
        print(f"   âœ“ Knowledge search preview successful")
        print(f"   - Results found: {len(preview_results)}")
        for i, result in enumerate(preview_results):
            print(f"   - Result {i+1}: {result['source_document']} (score: {result['relevance_score']})")
    else:
        print("   âœ— Knowledge search preview failed")
    
    # Test 2: Pipeline statistics
    print("\n2. Testing pipeline statistics:")
    
    stats = rag_pipeline.get_pipeline_statistics()
    
    if stats:
        print("   âœ“ Pipeline statistics retrieved")
        print(f"   - Pipeline enabled: {stats['pipeline_enabled']}")
        print(f"   - Selected collections: {stats['selected_collections_count']}")
        print(f"   - Max context length: {stats['max_context_length']}")
        print(f"   - Can process queries: {stats.get('can_process_queries', False)}")
    else:
        print("   âœ— Pipeline statistics retrieval failed")
    
    print("\n" + "=" * 50)
    print("âœ“ Knowledge search tests completed!")


def test_rag_pipeline_query_processing():
    """Test RAG pipeline query processing with mocked components."""
    print("\nTesting RAG Pipeline Query Processing...")
    print("=" * 50)
    
    # Create mock knowledge fragments
    mock_fragments = [
        KnowledgeFragment(
            content="äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ç”±æœºå™¨å±•ç¤ºçš„æ™ºèƒ½ï¼Œä¸äººç±»å±•ç¤ºçš„è‡ªç„¶æ™ºèƒ½å½¢æˆå¯¹æ¯”ã€‚AIç³»ç»Ÿå¯ä»¥æ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡ã€‚",
            source_document="ai_basics.pdf",
            collection_name="AI Knowledge",
            relevance_score=0.95,
            metadata={"document_type": "knowledge", "chunk_index": 0}
        ),
        KnowledgeFragment(
            content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹æ¥æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚",
            source_document="ml_guide.pdf",
            collection_name="AI Knowledge",
            relevance_score=0.87,
            metadata={"document_type": "knowledge", "chunk_index": 1}
        )
    ]
    
    # Create mock components
    mock_kb_manager = Mock()
    mock_kb_manager.search_knowledge.return_value = mock_fragments
    mock_kb_manager.list_collections.return_value = [
        Mock(id="coll_1", name="AI Knowledge")
    ]
    
    mock_llm_service = Mock()
    mock_llm_service.return_value = "åŸºäºæä¾›çš„çŸ¥è¯†å†…å®¹ï¼Œäººå·¥æ™ºèƒ½æ˜¯ç”±æœºå™¨å±•ç¤ºçš„æ™ºèƒ½ï¼Œå®ƒå¯ä»¥æ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡ã€‚æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªé‡è¦å­é›†ã€‚"
    
    rag_pipeline = RAGPipeline(
        knowledge_base_manager=mock_kb_manager,
        llm_service=mock_llm_service
    )
    rag_pipeline.enabled = True
    rag_pipeline.set_selected_collections(["coll_1"])
    
    # Test 1: Query processing with knowledge
    print("\n1. Testing query processing with knowledge:")
    
    query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    response = rag_pipeline.process_query_with_knowledge(query)
    
    if response and len(response) > 0:
        print("   âœ“ Query processed successfully with knowledge")
        print(f"   - Response length: {len(response)}")
        print(f"   - Response preview: {response[:100]}...")
        
        # Verify that search_knowledge was called
        mock_kb_manager.search_knowledge.assert_called()
        print("   âœ“ Knowledge search was invoked")
        
        # Verify that LLM service was called
        mock_llm_service.assert_called()
        print("   âœ“ LLM service was invoked")
    else:
        print("   âœ— Query processing failed")
    
    # Test 2: Query processing without collections
    print("\n2. Testing query processing without collections:")
    
    rag_pipeline.set_selected_collections([])
    response_no_collections = rag_pipeline.process_query_with_knowledge(query)
    
    if response_no_collections:
        print("   âœ“ Query processed without collections (fallback to standard)")
        print(f"   - Response length: {len(response_no_collections)}")
    else:
        print("   âœ— Query processing without collections failed")
    
    # Test 3: Query processing with disabled knowledge base
    print("\n3. Testing query processing with disabled knowledge base:")
    
    rag_pipeline.disable_knowledge_base()
    response_disabled = rag_pipeline.process_query_with_knowledge(query)
    
    if response_disabled:
        print("   âœ“ Query processed with disabled knowledge base")
        print(f"   - Response length: {len(response_disabled)}")
    else:
        print("   âœ— Query processing with disabled knowledge base failed")
    
    # Test 4: Empty query handling
    print("\n4. Testing empty query handling:")
    
    rag_pipeline.enable_knowledge_base()
    empty_response = rag_pipeline.process_query_with_knowledge("")
    
    if empty_response and "æœ‰æ•ˆçš„é—®é¢˜" in empty_response:
        print("   âœ“ Empty query handled correctly")
    else:
        print("   âœ— Empty query handling failed")
    
    print("\n" + "=" * 50)
    print("âœ“ Query processing tests completed!")


def test_rag_pipeline_error_handling():
    """Test RAG pipeline error handling."""
    print("\nTesting RAG Pipeline Error Handling...")
    print("=" * 50)
    
    # Test 1: Knowledge base manager failure
    print("\n1. Testing knowledge base manager failure:")
    
    mock_kb_manager = Mock()
    mock_kb_manager.search_knowledge.side_effect = Exception("Knowledge base connection failed")
    mock_kb_manager.list_collections.return_value = [Mock(id="coll_1")]
    
    mock_llm_service = Mock()
    mock_llm_service.return_value = "Standard response without knowledge"
    
    rag_pipeline = RAGPipeline(
        knowledge_base_manager=mock_kb_manager,
        llm_service=mock_llm_service
    )
    rag_pipeline.enabled = True
    rag_pipeline.set_selected_collections(["coll_1"])
    
    response = rag_pipeline.process_query_with_knowledge("Test query")
    
    if response and "çŸ¥è¯†åº“åŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨" in response:
        print("   âœ“ Knowledge base failure handled correctly")
        print(f"   - Fallback response provided: {len(response)} chars")
    else:
        print("   âœ— Knowledge base failure handling failed")
    
    # Test 2: LLM service failure
    print("\n2. Testing LLM service failure:")
    
    mock_kb_manager_working = Mock()
    mock_kb_manager_working.search_knowledge.return_value = []
    mock_kb_manager_working.list_collections.return_value = [Mock(id="coll_1")]
    
    mock_llm_service_failing = Mock()
    mock_llm_service_failing.side_effect = Exception("LLM service unavailable")
    
    rag_pipeline_llm_fail = RAGPipeline(
        knowledge_base_manager=mock_kb_manager_working,
        llm_service=mock_llm_service_failing
    )
    rag_pipeline_llm_fail.enabled = True
    
    response_llm_fail = rag_pipeline_llm_fail.process_query_with_knowledge("Test query")
    
    if response_llm_fail and "æ— æ³•å¤„ç†" in response_llm_fail:
        print("   âœ“ LLM service failure handled correctly")
    else:
        print("   âœ— LLM service failure handling failed")
    
    # Test 3: No LLM service available
    print("\n3. Testing no LLM service available:")
    
    rag_pipeline_no_llm = RAGPipeline()
    rag_pipeline_no_llm.enabled = False  # Force standard response path
    
    response_no_llm = rag_pipeline_no_llm.process_query_with_knowledge("Test query")
    
    if response_no_llm and "ä¸å¯ç”¨" in response_no_llm:
        print("   âœ“ No LLM service handled correctly")
    else:
        print("   âœ— No LLM service handling failed")
    
    print("\n" + "=" * 50)
    print("âœ“ Error handling tests completed!")


def run_all_tests():
    """Run all RAG pipeline tests."""
    print("Running All RAG Pipeline Tests")
    print("=" * 60)
    
    try:
        test_rag_pipeline_initialization()
        test_rag_pipeline_collection_management()
        test_rag_pipeline_status_and_control()
        test_rag_pipeline_prompt_building()
        test_rag_pipeline_knowledge_search()
        test_rag_pipeline_query_processing()
        test_rag_pipeline_error_handling()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ALL RAG PIPELINE TESTS COMPLETED SUCCESSFULLY! ğŸ‰")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ TEST SUITE FAILED: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()