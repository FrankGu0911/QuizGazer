import base64
import httpx
import os,time,logging
import json
from google.genai import Client, types
from langchain_openai import ChatOpenAI
from langchain.schema.messages import HumanMessage, SystemMessage
from utils.config_manager import get_model_config

# Import knowledge base components
try:
    from core.knowledge_base.rag_pipeline import RAGPipeline
    from core.knowledge_base.manager import KnowledgeBaseManager
    from core.knowledge_base.models import ChromaDBConfig
    from utils.config_manager import get_knowledge_base_config, get_chromadb_config
    KNOWLEDGE_BASE_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Knowledge base components not available: {e}")
    RAGPipeline = None
    KnowledgeBaseManager = None
    ChromaDBConfig = None
    get_knowledge_base_config = None
    get_chromadb_config = None
    KNOWLEDGE_BASE_AVAILABLE = False

# Global RAG pipeline instance
_rag_pipeline = None
_knowledge_base_manager = None

def initialize_knowledge_base():
    """Initialize the knowledge base and RAG pipeline."""
    global _rag_pipeline, _knowledge_base_manager
    
    if not KNOWLEDGE_BASE_AVAILABLE:
        logging.warning("Knowledge base components not available, skipping initialization")
        return False
    
    try:
        # Load configuration
        kb_config = get_knowledge_base_config() if get_knowledge_base_config else {}
        chromadb_config_dict = get_chromadb_config() if get_chromadb_config else {}
        
        # Note: We initialize the knowledge base components even if disabled in config
        # This allows users to enable it through the UI later
        # The enabled state will be handled by the RAG pipeline itself
        
        # Create ChromaDB configuration
        chromadb_config = ChromaDBConfig(
            connection_type=chromadb_config_dict.get('connection_type', 'local'),
            host=chromadb_config_dict.get('host'),
            port=chromadb_config_dict.get('port'),
            path=chromadb_config_dict.get('path', './data/chromadb'),
            auth_credentials=chromadb_config_dict.get('auth_credentials'),
            ssl_enabled=chromadb_config_dict.get('ssl_enabled', False)
        )
        
        # Initialize knowledge base manager
        storage_path = kb_config.get('storage_path', './data/knowledge_base')
        _knowledge_base_manager = KnowledgeBaseManager(
            storage_path=storage_path,
            chromadb_config=chromadb_config
        )
        
        # Initialize embedding service
        from core.knowledge_base.embedding_service import initialize_embedding_service
        embedding_initialized = initialize_embedding_service()
        if not embedding_initialized:
            logging.warning("Embedding service initialization failed, but continuing...")
        
        # Initialize RAG pipeline with LLM service integration
        _rag_pipeline = RAGPipeline(
            knowledge_base_manager=_knowledge_base_manager,
            llm_service=_get_llm_response_for_rag
        )
        
        logging.info("Knowledge base and RAG pipeline initialized successfully")
        return True
        
    except Exception as e:
        logging.error(f"Failed to initialize knowledge base: {e}")
        return False

def get_rag_pipeline():
    """Get the global RAG pipeline instance."""
    return _rag_pipeline

def get_knowledge_base_manager():
    """Get the global knowledge base manager instance."""
    return _knowledge_base_manager

def reload_knowledge_base_config():
    """é‡æ–°åŠ è½½çŸ¥è¯†åº“é…ç½®ï¼ˆç”¨äºé…ç½®ä¿®æ”¹åç«‹å³ç”Ÿæ•ˆï¼‰"""
    global _knowledge_base_manager
    
    if _knowledge_base_manager is None:
        print("âš ï¸ [AIæœåŠ¡] çŸ¥è¯†åº“ç®¡ç†å™¨æœªåˆå§‹åŒ–")
        return False
    
    try:
        print("ğŸ”„ [AIæœåŠ¡] é‡æ–°åŠ è½½çŸ¥è¯†åº“é…ç½®...")
        success = _knowledge_base_manager.reload_config()
        if success:
            print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“é…ç½®é‡è½½æˆåŠŸ")
        else:
            print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“é…ç½®é‡è½½å¤±è´¥")
        return success
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] çŸ¥è¯†åº“é…ç½®é‡è½½å¼‚å¸¸: {e}")
        return False

def is_knowledge_base_available():
    """Check if knowledge base is available and initialized."""
    return KNOWLEDGE_BASE_AVAILABLE and _rag_pipeline is not None

def _get_llm_response_for_rag(prompt: str) -> str:
    """
    Internal function to get LLM response for RAG pipeline.
    This wraps the existing get_answer_from_text function.
    
    IMPORTANT: use_knowledge_base=False to prevent recursive calls!
    """
    try:
        print("ğŸ”„ [AIæœåŠ¡] RAGç®¡é“è°ƒç”¨LLMæœåŠ¡ï¼ˆç¦ç”¨çŸ¥è¯†åº“ä»¥é˜²é€’å½’ï¼‰")
        return get_answer_from_text(prompt, force_search=False, use_knowledge_base=False)
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] RAGçš„LLMè°ƒç”¨å¤±è´¥: {e}")
        logging.error(f"Error getting LLM response for RAG: {e}")
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

def _extract_longest_json(text):
    """
    ä»æ–‡æœ¬ä¸­æå–æœ€é•¿çš„JSONç»“æ„ï¼Œä¼˜å…ˆåŒ¹é…[]æ•°ç»„ï¼Œç„¶ååŒ¹é…{}å¯¹è±¡

    Args:
        text (str): åŒ…å«JSONçš„æ–‡æœ¬

    Returns:
        str: æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    def find_matching_bracket(text, start_pos, open_char, close_char):
        """æ‰¾åˆ°åŒ¹é…çš„æ‹¬å·ä½ç½®"""
        count = 1
        pos = start_pos + 1
        while pos < len(text) and count > 0:
            if text[pos] == open_char:
                count += 1
            elif text[pos] == close_char:
                count -= 1
            pos += 1
        return pos - 1 if count == 0 else -1

    # ä¼˜å…ˆæŸ¥æ‰¾æœ€é•¿çš„[]æ•°ç»„
    longest_array = ""
    for i, char in enumerate(text):
        if char == '[':
            end_pos = find_matching_bracket(text, i, '[', ']')
            if end_pos != -1:
                candidate = text[i:end_pos + 1]
                if len(candidate) > len(longest_array):
                    longest_array = candidate

    if longest_array:
        return longest_array

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°ç»„ï¼ŒæŸ¥æ‰¾æœ€é•¿çš„{}å¯¹è±¡
    longest_object = ""
    for i, char in enumerate(text):
        if char == '{':
            end_pos = find_matching_bracket(text, i, '{', '}')
            if end_pos != -1:
                candidate = text[i:end_pos + 1]
                if len(candidate) > len(longest_object):
                    longest_object = candidate

    return longest_object if longest_object else None

def get_direct_answer_from_image(image_bytes, force_search=False):
    """
    ç›´æ¥ä»å›¾åƒè·å–ç­”æ¡ˆï¼Œé€‚ç”¨äºåŒ…å«å›¾å½¢ã€å‡½æ•°å›¾ã€å‡ ä½•å›¾ç­‰è§†è§‰å…ƒç´ çš„é¢˜ç›®
    ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ä¸€æ­¥åˆ°ä½è§£ç­”é—®é¢˜

    Args:
        image_bytes (bytes): The image data in bytes.
        force_search (bool): Whether to force the model to use search tools.

    Returns:
        str: The answer from the multimodal model, or an error message.
    """
    llm_config = get_model_config('llm')
    if not llm_config:
        return "Error: LLM configuration is missing or invalid."

    provider = llm_config.get('llm_provider', 'gemini')

    if provider == 'gemini':
        try:
            client_options = {
                "api_key": llm_config.get('google_api_key') or llm_config.get('api_key')
            }
            # æ³¨æ„ï¼šå¯¹äºåŸç”Ÿ Google GenAI APIï¼Œä¸éœ€è¦è®¾ç½® http_options
            client_options["http_options"] = {'base_url': llm_config.get('base_url',"https://generativelanguage.googleapis.com/v1beta/openai/")}

            genai_client = Client(**client_options)

            model_name = llm_config.get('model_name', 'gemini-2.5-flash')
            cur_time = time.strftime("%Y-%m-%d_%H-%M-%S")

            system_instruction = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”é¢˜åŠ©æ‰‹ã€‚ç°åœ¨æ—¶é—´æ˜¯{cur_time}ã€‚è¯·ç›´æ¥åˆ†æå›¾ç‰‡ä¸­çš„é—®é¢˜å¹¶ç»™å‡ºç­”æ¡ˆã€‚

å¯¹äºé€‰æ‹©é¢˜ï¼š
1. é¦–å…ˆç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š**ç­”æ¡ˆï¼šA** æˆ– **ç­”æ¡ˆï¼šAã€C**ï¼ˆå¤šé€‰é¢˜ï¼‰
2. ç„¶åç®€æ˜æ‰¼è¦è¯´æ˜ç†ç”±ï¼Œä¸è¦é•¿ç¯‡å¤§è®º

å¯¹äºä¸»è§‚é¢˜ï¼š
ç›´æ¥ç»™å‡ºå‡†ç¡®çš„ä¸­æ–‡ç­”æ¡ˆå’Œè§£é‡Š

ç‰¹åˆ«æ³¨æ„ï¼š
- ä»”ç»†è§‚å¯Ÿå›¾ç‰‡ä¸­çš„æ‰€æœ‰è§†è§‰å…ƒç´ ï¼ŒåŒ…æ‹¬å›¾å½¢ã€å›¾è¡¨ã€å‡½æ•°å›¾åƒã€å‡ ä½•å›¾å½¢ç­‰
- å¦‚æœé¢˜ç›®æ¶‰åŠå›¾å½¢åˆ†æï¼Œè¯·åŸºäºå›¾ç‰‡ä¸­çš„è§†è§‰ä¿¡æ¯è¿›è¡Œè§£ç­”
- å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”
- é€‰æ‹©é¢˜å¿…é¡»æ˜ç¡®æŒ‡å‡ºé€‰é¡¹å­—æ¯
- å›ç­”è¦å‡†ç¡®ã€ç®€æ´"""

            if force_search:
                system_instruction += " å¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·å¯»æ‰¾ç­”æ¡ˆ"

            # æ£€æµ‹å›¾ç‰‡æ ¼å¼
            def detect_image_mime_type(image_data):
                """æ£€æµ‹å›¾ç‰‡çš„MIMEç±»å‹"""
                if image_data.startswith(b'\x89PNG'):
                    return 'image/png'
                elif image_data.startswith(b'\xff\xd8\xff'):
                    return 'image/jpeg'
                elif image_data.startswith(b'GIF'):
                    return 'image/gif'
                elif image_data.startswith(b'RIFF') and b'WEBP' in image_data[:12]:
                    return 'image/webp'
                else:
                    return 'image/png'  # é»˜è®¤ä½¿ç”¨PNG

            mime_type = detect_image_mime_type(image_bytes)

            # ä½¿ç”¨æ­£ç¡®çš„ Google GenAI API æ ¼å¼æ„å»ºå†…å®¹
            content = [
                types.Part.from_bytes(
                    data=image_bytes,
                    mime_type=mime_type,
                ),
                "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„é—®é¢˜å¹¶ç»™å‡ºç­”æ¡ˆã€‚å¦‚æœå›¾ç‰‡åŒ…å«å›¾å½¢ã€å›¾è¡¨æˆ–å…¶ä»–è§†è§‰å…ƒç´ ï¼Œè¯·åŸºäºè¿™äº›è§†è§‰ä¿¡æ¯è¿›è¡Œåˆ†æã€‚"
            ]

            # æ„å»ºé…ç½®
            config_kwargs = {
                "system_instruction": system_instruction,
            }

            # åªæœ‰åœ¨éœ€è¦æœç´¢æ—¶æ‰æ·»åŠ å·¥å…·
            if force_search:
                config_kwargs["tools"] = [
                    types.Tool(
                        google_search=types.GoogleSearch()
                    )
                ]

            response = genai_client.models.generate_content(
                model=model_name,
                contents=content,
                config=types.GenerateContentConfig(**config_kwargs),
            )

            print(f"Response from Multimodal LLM: {response}")
            return response.text

        except Exception as e:
            return f"An error occurred while communicating with the Gemini API: {e}"

    else:
        # å¯¹äºéGeminiæä¾›å•†ï¼Œå›é€€åˆ°ä¸¤æ­¥èµ°æ–¹æ¡ˆ
        return "Error: Direct image answering is only supported with Gemini provider. Please use the two-step approach (extract question first, then answer)."

def get_question_from_image(image_bytes):
    """
    Sends an image to the configured VLM to extract a question.

    Args:
        image_bytes (bytes): The image data in bytes.

    Returns:
        str: The extracted question text, or an error message.
    """
    vlm_config = get_model_config('vlm')
    if not vlm_config:
        return "Error: VLM configuration is missing or invalid."

    # Encode the image in base64
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    # Set up httpx client with timeout and proxy if available
    client_kwargs = {"timeout": 30}
    if vlm_config.get('proxy'):
        client_kwargs['proxies'] = vlm_config['proxy']
    
    http_client = httpx.Client(**client_kwargs)

    chat = ChatOpenAI(
        model=vlm_config['model_name'],
        openai_api_key=vlm_config['api_key'],
        base_url=vlm_config['base_url'],
        http_client=http_client
    )

    try:
        response = chat.invoke(
            [
                SystemMessage(
                    content='''
ä»¥JSONæ•°ç»„æ ¼å¼æå–å›¾ä¸­æ‰€æœ‰é—®é¢˜ã€‚

æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«`question_text`(å­—ç¬¦ä¸²), `code_block`(å­—ç¬¦ä¸²æˆ–`null`), `options`(å­—ç¬¦ä¸²æ•°ç»„`[]`)ä»¥åŠ`question_type`å››ä¸ªé”®ã€‚
`question_type`çš„åˆ¤æ–­è§„åˆ™å¦‚ä¸‹ï¼š
- å¦‚æœé—®é¢˜æ–‡æœ¬åŒ…å«â€œå¤šé€‰â€ç­‰å…³é”®è¯ï¼Œæˆ–æ˜ç¡®æç¤ºå¯é€‰å¤šä¸ªç­”æ¡ˆï¼Œåˆ™ä¸º "å¤šé€‰é¢˜"ã€‚
- å¦‚æœ`options`æ•°ç»„ä¸ºç©ºï¼Œåˆ™ä¸º "ä¸»è§‚é¢˜"ã€‚
- å…¶ä»–æ‰€æœ‰æœ‰é€‰é¡¹çš„æƒ…å†µï¼Œé»˜è®¤ä¸º "å•é€‰é¢˜"ã€‚

å¯¹äºé€‰é¡¹æå–çš„é‡è¦è§„åˆ™ï¼š
1. å¿…é¡»åŒæ—¶åŒ…å«é€‰é¡¹æ ‡è¯†ï¼ˆå¦‚Aã€Bã€Cã€Dï¼‰å’Œé€‰é¡¹å†…å®¹
2. æ ¼å¼åº”ä¸ºï¼š"A. é€‰é¡¹å†…å®¹" æˆ– "Aã€é€‰é¡¹å†…å®¹" æˆ– "Aï¼šé€‰é¡¹å†…å®¹" ç­‰
3. å³ä½¿åŸé¢˜ä¸­é€‰é¡¹æ ‡è¯†å’Œå†…å®¹æ˜¯åˆ†ç¦»çš„ï¼Œä¹Ÿå¿…é¡»å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´é€‰é¡¹


ç¤ºä¾‹:
```json
[
  {
    "question_text": "é¢˜ç›®çš„ä¸»è¦æ–‡æœ¬",
    "code_block": "å®Œæ•´çš„ä»£ç å†…å®¹",
    "options": ["A. ç¬¬ä¸€ä¸ªé€‰é¡¹å†…å®¹", "B. ç¬¬äºŒä¸ªé€‰é¡¹å†…å®¹", "C. ç¬¬ä¸‰ä¸ªé€‰é¡¹å†…å®¹", "D. ç¬¬å››ä¸ªé€‰é¡¹å†…å®¹"],
    "question_type": "å•é€‰é¢˜"
  }
]
````

ä½ çš„å›ç­”å¿…é¡»åªåŒ…å«JSONæ•°ç»„ï¼Œæ— ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚è‹¥å›¾ä¸­æ— é—®é¢˜ï¼Œåˆ™è¿”å›ç©ºæ•°ç»„`[]`ã€‚
                    '''
                ),
                HumanMessage(
                    content=[
                        {"type": "text", "text": "Extract the question from this image."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            },
                        },
                    ]
                ),
            ]
        )
        print(f"Response from VLM: {response}")

        # éªŒè¯è¿”å›çš„å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSON
        response_content = response.content.strip()

        # å…ˆå°è¯•æå–æœ€é•¿çš„JSONç»“æ„
        json_content = _extract_longest_json(response_content)
        if not json_content:
            return f"Error: No valid JSON structure found in VLM response: {response_content}"

        try:
            # å°è¯•è§£ææå–çš„JSON
            parsed_json = json.loads(json_content)

            # å¦‚æœæ˜¯ç©ºæ•°ç»„ï¼Œç›´æ¥è¿”å›
            if isinstance(parsed_json, list) and len(parsed_json) == 0:
                return "[]"

            # å¦‚æœè§£ææˆåŠŸä¸”ä¸ä¸ºç©ºï¼Œè¿”å›æå–çš„JSONå†…å®¹
            return json_content

        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return f"Error: VLM returned invalid JSON format: {response_content}"

    except Exception as e:
        return f"An error occurred while communicating with the VLM: {e}"


def get_answer_from_text(question_text, force_search=False, use_knowledge_base=True):
    """
    Sends a question text to the configured LLM to get an answer.
    Optionally uses knowledge base for enhanced responses.

    Args:
        question_text (str): The question to be answered.
        force_search (bool): Whether to force search tools usage.
        use_knowledge_base (bool): Whether to use knowledge base for enhanced answers.

    Returns:
        str: The answer from the LLM, or an error message.
    """
    # Try to use knowledge base first if available and enabled
    print(f"ğŸ¤– [AIæœåŠ¡] get_answer_from_text è°ƒç”¨ï¼Œuse_knowledge_base={use_knowledge_base}")
    if use_knowledge_base and is_knowledge_base_available():
        print("ğŸš€ [AIæœåŠ¡] çŸ¥è¯†åº“å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨RAGå¢å¼ºå“åº”")
        try:
            rag_pipeline = get_rag_pipeline()
            if rag_pipeline and rag_pipeline.should_use_knowledge_base():
                print("âœ… [AIæœåŠ¡] RAGç®¡é“å¯ç”¨ä¸”åº”è¯¥ä½¿ç”¨çŸ¥è¯†åº“")
                print("ğŸ” [AIæœåŠ¡] è°ƒç”¨RAGç®¡é“å¤„ç†æŸ¥è¯¢...")
                logging.info("Using knowledge base for enhanced response")
                enhanced_response = rag_pipeline.process_query_with_knowledge(question_text)
                if enhanced_response and not enhanced_response.startswith("æŠ±æ­‰"):
                    print("âœ… [AIæœåŠ¡] RAGå¢å¼ºå“åº”æˆåŠŸï¼Œè¿”å›ç»“æœ")
                    print(f"ğŸ“ [AIæœåŠ¡] å“åº”é•¿åº¦: {len(enhanced_response)} å­—ç¬¦")
                    return enhanced_response
                else:
                    print("âš ï¸ [AIæœåŠ¡] RAGå“åº”ä¸æ»¡æ„ï¼Œå›é€€åˆ°æ ‡å‡†LLM")
                    logging.info("Knowledge base response not satisfactory, falling back to standard LLM")
            else:
                print("âŒ [AIæœåŠ¡] RAGç®¡é“ä¸å¯ç”¨æˆ–ä¸åº”ä½¿ç”¨çŸ¥è¯†åº“")
        except Exception as e:
            print(f"âŒ [AIæœåŠ¡] ä½¿ç”¨çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            logging.error(f"Error using knowledge base: {e}")
            logging.info("Falling back to standard LLM response")
    else:
        if not use_knowledge_base:
            print("ğŸ”’ [AIæœåŠ¡] çŸ¥è¯†åº“ä½¿ç”¨è¢«ç¦ç”¨")
        else:
            print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨")
    
    # Standard LLM processing (original implementation)
    llm_config = get_model_config('llm')
    if not llm_config:
        return "Error: LLM configuration is missing or invalid."

    provider = llm_config.get('llm_provider', 'gemini')
    chat = None

    if provider == 'gemini':
        try:
            client_options = {
                "api_key": llm_config.get('google_api_key') or llm_config.get('api_key')
            }
            client_options["http_options"] = {'base_url': llm_config.get('base_url',"https://generativelanguage.googleapis.com/v1beta/openai/")}

            genai_client = Client(**client_options)
            
            model_name = llm_config.get('model_name', 'gemini-2.5-flash')
            # model = genai.GenerativeModel(model_name)
            cur_time = time.strftime("%Y-%m-%d_%H-%M-%S")
            system_instruction = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”é¢˜åŠ©æ‰‹ã€‚ç°åœ¨æ—¶é—´æ˜¯{cur_time}ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š
            å¯¹äºé€‰æ‹©é¢˜ï¼š
            1. é¦–å…ˆç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š**ç­”æ¡ˆï¼šA** æˆ– **ç­”æ¡ˆï¼šAã€C**ï¼ˆå¤šé€‰é¢˜ï¼‰
            2. ç„¶åç®€æ˜æ‰¼è¦è¯´æ˜ç†ç”±ï¼Œä¸è¦é•¿ç¯‡å¤§è®º

            å¯¹äºä¸»è§‚é¢˜ï¼š
            ç›´æ¥ç»™å‡ºå‡†ç¡®çš„ä¸­æ–‡ç­”æ¡ˆå’Œè§£é‡Š

            è¦æ±‚ï¼š
            - å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”
            - é€‰æ‹©é¢˜å¿…é¡»æ˜ç¡®æŒ‡å‡ºé€‰é¡¹å­—æ¯
            - å›ç­”è¦å‡†ç¡®ã€ç®€æ´
            - æ³¨æ„é¢˜ç›®å¯èƒ½æ¥è‡ªOCRï¼Œå­˜åœ¨è¯†åˆ«é”™è¯¯ï¼Œè¯·åˆç†åˆ¤æ–­"""
            if force_search:
                system_instruction += " å¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·å¯»æ‰¾ç­”æ¡ˆ"
            content = question_text
            response = genai_client.models.generate_content(
                model=model_name,
                contents=content,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    tools=[
                        types.Tool(
                            google_search=types.GoogleSearch()
                        )
                    ],
                    
                ),
            )
            
            print(f"Response from LLM: {response}")
            return response.text
        except Exception as e:
            return f"An error occurred while communicating with the Gemini API: {e}"

    else:
        # For other providers, use the existing ChatOpenAI setup.
        # Set up httpx client with timeout and proxy if available
        client_kwargs = {"timeout": 30}
        if llm_config.get('proxy'):
            client_kwargs['proxies'] = llm_config['proxy']
            
        http_client = httpx.Client(**client_kwargs)

        chat = ChatOpenAI(
            model=llm_config['model_name'],
            openai_api_key=llm_config['api_key'],
            base_url=llm_config['base_url'],
            http_client=http_client
        )
        try:
            system_prompt = "You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡åŠ©æ‰‹ã€‚æ— è®ºç”¨æˆ·æé—®ä½¿ç”¨ä»€ä¹ˆè¯­è¨€ï¼Œä½ éƒ½å¿…é¡»å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚å¯¹äºé€‰æ‹©é¢˜ï¼ˆå•é€‰æˆ–å¤šé€‰ï¼‰ï¼Œè¯·å…ˆåˆ†æé—®é¢˜ï¼Œç„¶åæ˜ç¡®æŒ‡å‡ºæ­£ç¡®ç­”æ¡ˆçš„é€‰é¡¹ï¼ˆå¦‚'ç­”æ¡ˆæ˜¯B'æˆ–'ç­”æ¡ˆæ˜¯Aå’ŒC'ï¼‰ã€‚ä¹‹åå†æä¾›è¯¦ç»†è§£é‡Šã€‚Provide a concise and accurate answer to the user's question. å³ä½¿é¢˜ç›®å…¨æ˜¯è‹±æ–‡ï¼Œä½ ä¹Ÿå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ã€‚"
            if force_search:
                logging.warning("OpenAI compatibility mode: force_search is not usable")
                # system_prompt += " å¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·å¯»æ‰¾ç­”æ¡ˆ"
            response = chat.invoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=question_text),
                ]
            )
            print(f"Response from LLM: {response}")
            return response.content
        except Exception as e:
            return f"An error occurred while communicating with the LLM: {e}"

# Knowledge Base Management Functions

def get_knowledge_base_status():
    """
    Get the current status of the knowledge base.
    
    Returns:
        dict: Knowledge base status information
    """
    if not is_knowledge_base_available():
        return {
            "available": False,
            "enabled": False,
            "error": "Knowledge base not initialized or not available"
        }
    
    try:
        rag_pipeline = get_rag_pipeline()
        return rag_pipeline.get_knowledge_base_status()
    except Exception as e:
        return {
            "available": False,
            "enabled": False,
            "error": str(e)
        }

def set_knowledge_base_collections(collection_ids):
    """
    Set the collections to use for knowledge base queries.
    
    Args:
        collection_ids (list): List of collection IDs to use
        
    Returns:
        bool: True if successful, False otherwise
    """
    if not is_knowledge_base_available():
        return False
    
    try:
        rag_pipeline = get_rag_pipeline()
        rag_pipeline.set_selected_collections(collection_ids)
        return True
    except Exception as e:
        logging.error(f"Error setting knowledge base collections: {e}")
        return False

def get_knowledge_base_collections():
    """
    Get available knowledge base collections.
    
    Returns:
        list: List of available collections
    """
    if not is_knowledge_base_available():
        return []
    
    try:
        kb_manager = get_knowledge_base_manager()
        return kb_manager.list_collections()
    except Exception as e:
        logging.error(f"Error getting knowledge base collections: {e}")
        return []

def enable_knowledge_base():
    """
    Enable knowledge base functionality.
    
    Returns:
        bool: True if successful, False otherwise
    """
    print("ğŸš€ [AIæœåŠ¡] å¯ç”¨çŸ¥è¯†åº“åŠŸèƒ½...")
    
    if not is_knowledge_base_available():
        print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨ï¼Œæ— æ³•å¯ç”¨")
        return False
    
    try:
        print("ğŸ”§ [AIæœåŠ¡] è·å–RAGç®¡é“å®ä¾‹...")
        rag_pipeline = get_rag_pipeline()
        
        print("âš¡ [AIæœåŠ¡] è°ƒç”¨RAGç®¡é“çš„å¯ç”¨çŸ¥è¯†åº“æ–¹æ³•...")
        rag_pipeline.enable_knowledge_base()
        
        print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“åŠŸèƒ½å¯ç”¨æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] å¯ç”¨çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(f"ğŸ” [AIæœåŠ¡] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logging.error(f"Error enabling knowledge base: {e}")
        return False

def disable_knowledge_base():
    """
    Disable knowledge base functionality.
    
    Returns:
        bool: True if successful, False otherwise
    """
    print("ğŸ›‘ [AIæœåŠ¡] ç¦ç”¨çŸ¥è¯†åº“åŠŸèƒ½...")
    
    if not is_knowledge_base_available():
        print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨ï¼Œæ— æ³•ç¦ç”¨")
        return False
    
    try:
        print("ğŸ”§ [AIæœåŠ¡] è·å–RAGç®¡é“å®ä¾‹...")
        rag_pipeline = get_rag_pipeline()
        
        print("âš¡ [AIæœåŠ¡] è°ƒç”¨RAGç®¡é“çš„ç¦ç”¨çŸ¥è¯†åº“æ–¹æ³•...")
        rag_pipeline.disable_knowledge_base()
        
        print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“åŠŸèƒ½ç¦ç”¨æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] ç¦ç”¨çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(f"ğŸ” [AIæœåŠ¡] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logging.error(f"Error disabling knowledge base: {e}")
        return False

def search_knowledge_preview(query, collections=None, top_k=3):
    """
    Search knowledge base and return preview results.
    
    Args:
        query (str): Search query
        collections (list): Collections to search (None for all selected)
        top_k (int): Maximum number of results
        
    Returns:
        list: List of knowledge fragment previews
    """
    if not is_knowledge_base_available():
        return []
    
    try:
        rag_pipeline = get_rag_pipeline()
        return rag_pipeline.search_knowledge_preview(query, collections, top_k)
    except Exception as e:
        logging.error(f"Error searching knowledge base: {e}")
        return []

def sync_knowledge_base_metadata():
    """
    Sync knowledge base metadata from remote ChromaDB.
    
    Returns:
        bool: True if sync was successful, False otherwise
    """
    print("ğŸ”„ [AIæœåŠ¡] å¼€å§‹åŒæ­¥çŸ¥è¯†åº“å…ƒæ•°æ®...")
    
    if not is_knowledge_base_available():
        print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨ï¼Œæ— æ³•åŒæ­¥å…ƒæ•°æ®")
        return False
    
    try:
        print("ğŸ”§ [AIæœåŠ¡] è·å–çŸ¥è¯†åº“ç®¡ç†å™¨å®ä¾‹...")
        kb_manager = get_knowledge_base_manager()
        
        print("âš¡ [AIæœåŠ¡] è°ƒç”¨å…ƒæ•°æ®åŒæ­¥æ–¹æ³•...")
        success = kb_manager.sync_metadata_from_remote()
        
        if success:
            print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“å…ƒæ•°æ®åŒæ­¥æˆåŠŸ")
        else:
            print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“å…ƒæ•°æ®åŒæ­¥å¤±è´¥")
        
        return success
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] åŒæ­¥çŸ¥è¯†åº“å…ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(f"ğŸ” [AIæœåŠ¡] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logging.error(f"Error syncing knowledge base metadata: {e}")
        return False

def refresh_knowledge_base():
    """
    åˆ·æ–°çŸ¥è¯†åº“æ•°æ®ï¼ˆä¸éœ€è¦é‡å¯åº”ç”¨ï¼‰
    
    Returns:
        dict: åˆ·æ–°ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
    """
    print("ğŸ”„ [AIæœåŠ¡] åˆ·æ–°çŸ¥è¯†åº“æ•°æ®...")
    
    result = {
        "success": False,
        "message": "",
        "stats_before": {},
        "stats_after": {},
        "changes": {}
    }
    
    try:
        # è·å–åˆ·æ–°å‰çš„ç»Ÿè®¡ä¿¡æ¯
        result["stats_before"] = get_knowledge_base_statistics()
        
        # æ‰§è¡ŒåŒæ­¥
        sync_success = sync_knowledge_base_metadata()
        
        if sync_success:
            # è·å–åˆ·æ–°åçš„ç»Ÿè®¡ä¿¡æ¯
            result["stats_after"] = get_knowledge_base_statistics()
            
            # è®¡ç®—å˜åŒ–
            before = result["stats_before"]
            after = result["stats_after"]
            
            if "error" not in before and "error" not in after:
                result["changes"] = {
                    "collections": after.get("total_collections", 0) - before.get("total_collections", 0),
                    "documents": after.get("total_documents", 0) - before.get("total_documents", 0),
                    "chunks": after.get("total_chunks", 0) - before.get("total_chunks", 0)
                }
            
            result["success"] = True
            result["message"] = "çŸ¥è¯†åº“æ•°æ®åˆ·æ–°æˆåŠŸ"
            print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“æ•°æ®åˆ·æ–°å®Œæˆ")
        else:
            result["message"] = "çŸ¥è¯†åº“å…ƒæ•°æ®åŒæ­¥å¤±è´¥"
            print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“æ•°æ®åˆ·æ–°å¤±è´¥")
        
        return result
        
    except Exception as e:
        result["message"] = f"åˆ·æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ [AIæœåŠ¡] åˆ·æ–°çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        logging.error(f"Error refreshing knowledge base: {e}")
        return result

def debug_task_status(task_id: str):
    """
    è°ƒè¯•ä»»åŠ¡çŠ¶æ€ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸ä¸€è‡´é—®é¢˜
    
    Args:
        task_id: ä»»åŠ¡ID
        
    Returns:
        dict: ä»»åŠ¡çŠ¶æ€è°ƒè¯•ä¿¡æ¯
    """
    print(f"ğŸ” [AIæœåŠ¡] è°ƒè¯•ä»»åŠ¡çŠ¶æ€: {task_id}")
    
    if not is_knowledge_base_available():
        return {"error": "Knowledge base not available"}
    
    try:
        kb_manager = get_knowledge_base_manager()
        if hasattr(kb_manager, 'task_manager'):
            validation = kb_manager.task_manager.validate_task_consistency(task_id)
            
            print(f"ğŸ“Š [AIæœåŠ¡] ä»»åŠ¡çŠ¶æ€è°ƒè¯•ç»“æœ:")
            print(f"   - çŠ¶æ€: {validation.get('status')}")
            print(f"   - è¿›åº¦: {validation.get('progress')}")
            print(f"   - å—æ•°é‡: {validation.get('chunk_count')}")
            
            if validation.get('inconsistencies'):
                print(f"âš ï¸ [AIæœåŠ¡] å‘ç°ä¸ä¸€è‡´é—®é¢˜:")
                for issue in validation['inconsistencies']:
                    print(f"     - {issue}")
            else:
                print(f"âœ… [AIæœåŠ¡] ä»»åŠ¡çŠ¶æ€ä¸€è‡´")
            
            return validation
        else:
            return {"error": "Task manager not available"}
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] è°ƒè¯•ä»»åŠ¡çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        logging.error(f"Error debugging task status: {e}")
        return {"error": str(e)}

def get_knowledge_base_statistics():
    """
    Get knowledge base statistics.
    
    Returns:
        dict: Knowledge base statistics
    """
    if not is_knowledge_base_available():
        return {"error": "Knowledge base not available"}
    
    try:
        kb_manager = get_knowledge_base_manager()
        stats = kb_manager.get_knowledge_base_stats()
        
        # Add task statistics if available
        if hasattr(kb_manager, 'task_manager'):
            task_stats = kb_manager.task_manager.get_task_statistics()
            stats["task_statistics"] = task_stats
            
            # Highlight any inconsistent tasks
            if task_stats.get("inconsistent_tasks", 0) > 0:
                print(f"âš ï¸ [AIæœåŠ¡] å‘ç° {task_stats['inconsistent_tasks']} ä¸ªçŠ¶æ€ä¸ä¸€è‡´çš„ä»»åŠ¡")
        
        return stats
    except Exception as e:
        logging.error(f"Error getting knowledge base statistics: {e}")
        return {"error": str(e)}

# Initialize knowledge base on module import
try:
    initialize_knowledge_base()
except Exception as e:
    logging.error(f"Failed to initialize knowledge base on module import: {e}")


# å†å²è®°å½•ä¸Šä¼ åŠŸèƒ½
async def upload_quiz_record(image_bytes, question_text, answer_text,
                           ocr_time, answer_time, model_info):
    """
    ä¸Šä¼ æµ‹éªŒè®°å½•åˆ°åç«¯æœåŠ¡å™¨

    Args:
        image_bytes: å›¾ç‰‡æ•°æ®
        question_text: è¯†åˆ«çš„é¢˜ç›®æ–‡æœ¬
        answer_text: ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬
        ocr_time: OCR å¤„ç†æ—¶é—´
        answer_time: ç­”æ¡ˆç”Ÿæˆæ—¶é—´
        model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸

    Returns:
        bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–åç«¯é…ç½®
        from utils.config_manager import get_backend_config
        backend_config = get_backend_config()
        
        if not backend_config.get('enable_history', False):
            print("ğŸ“ [å†å²è®°å½•] å†å²è®°å½•åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡ä¸Šä¼ ")
            return True  # å¦‚æœæœªå¯ç”¨å†å²è®°å½•ï¼Œç›´æ¥è¿”å›æˆåŠŸ

        base_url = backend_config.get('base_url', 'http://localhost:8000')
        user_id = backend_config.get('user_id', '')

        print(f"ğŸ“¤ [å†å²è®°å½•] å¼€å§‹ä¸Šä¼ åˆ° {base_url}")

        # åˆ›å»º multipart form data
        files = {
            'image': ('quiz_screenshot.png', image_bytes, 'image/png'),
        }

        from datetime import datetime
        data = {
            'question_text': question_text,
            'answer_text': answer_text,
            'vlm_model': model_info.get('vlm_model', ''),
            'llm_model': model_info.get('llm_model', ''),
            'ocr_time': str(ocr_time),
            'answer_time': str(answer_time),
            'user_id': user_id,
            'session_id': f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        }

        # å¼‚æ­¥ä¸Šä¼ 
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(
                f"{base_url}/api/quiz/record-with-image",
                files=files,
                data=data
            )

            if response.status_code == 200:
                print("âœ… [å†å²è®°å½•] ä¸Šä¼ æˆåŠŸ")
                return True
            else:
                print(f"âŒ [å†å²è®°å½•] ä¸Šä¼ å¤±è´¥: {response.status_code} - {response.text}")
                return False

    except Exception as e:
        print(f"âŒ [å†å²è®°å½•] ä¸Šä¼ æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False