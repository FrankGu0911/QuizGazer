import base64
import httpx
import os,time,logging
import json
from google.genai import Client, types
from langchain_openai import ChatOpenAI
from langchain.schema.messages import HumanMessage, SystemMessage
from utils.config_manager import get_model_config

# Import knowledge base components
try:
    from core.knowledge_base.rag_pipeline import RAGPipeline
    from core.knowledge_base.manager import KnowledgeBaseManager
    from core.knowledge_base.models import ChromaDBConfig
    from utils.config_manager import get_knowledge_base_config, get_chromadb_config
    KNOWLEDGE_BASE_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Knowledge base components not available: {e}")
    RAGPipeline = None
    KnowledgeBaseManager = None
    ChromaDBConfig = None
    get_knowledge_base_config = None
    get_chromadb_config = None
    KNOWLEDGE_BASE_AVAILABLE = False

# Global RAG pipeline instance
_rag_pipeline = None
_knowledge_base_manager = None

def initialize_knowledge_base():
    """Initialize the knowledge base and RAG pipeline."""
    global _rag_pipeline, _knowledge_base_manager
    
    if not KNOWLEDGE_BASE_AVAILABLE:
        logging.warning("Knowledge base components not available, skipping initialization")
        return False
    
    try:
        # Load configuration
        kb_config = get_knowledge_base_config() if get_knowledge_base_config else {}
        chromadb_config_dict = get_chromadb_config() if get_chromadb_config else {}
        
        # Note: We initialize the knowledge base components even if disabled in config
        # This allows users to enable it through the UI later
        # The enabled state will be handled by the RAG pipeline itself
        
        # Create ChromaDB configuration
        chromadb_config = ChromaDBConfig(
            connection_type=chromadb_config_dict.get('connection_type', 'local'),
            host=chromadb_config_dict.get('host'),
            port=chromadb_config_dict.get('port'),
            path=chromadb_config_dict.get('path', './data/chromadb'),
            auth_credentials=chromadb_config_dict.get('auth_credentials'),
            ssl_enabled=chromadb_config_dict.get('ssl_enabled', False)
        )
        
        # Initialize knowledge base manager
        storage_path = kb_config.get('storage_path', './data/knowledge_base')
        _knowledge_base_manager = KnowledgeBaseManager(
            storage_path=storage_path,
            chromadb_config=chromadb_config
        )
        
        # Initialize embedding service
        from core.knowledge_base.embedding_service import initialize_embedding_service
        embedding_initialized = initialize_embedding_service()
        if not embedding_initialized:
            logging.warning("Embedding service initialization failed, but continuing...")
        
        # Initialize RAG pipeline with LLM service integration
        _rag_pipeline = RAGPipeline(
            knowledge_base_manager=_knowledge_base_manager,
            llm_service=_get_llm_response_for_rag
        )
        
        logging.info("Knowledge base and RAG pipeline initialized successfully")
        return True
        
    except Exception as e:
        logging.error(f"Failed to initialize knowledge base: {e}")
        return False

def get_rag_pipeline():
    """Get the global RAG pipeline instance."""
    return _rag_pipeline

def get_knowledge_base_manager():
    """Get the global knowledge base manager instance."""
    return _knowledge_base_manager

def is_knowledge_base_available():
    """Check if knowledge base is available and initialized."""
    return KNOWLEDGE_BASE_AVAILABLE and _rag_pipeline is not None

def _get_llm_response_for_rag(prompt: str) -> str:
    """
    Internal function to get LLM response for RAG pipeline.
    This wraps the existing get_answer_from_text function.
    
    IMPORTANT: use_knowledge_base=False to prevent recursive calls!
    """
    try:
        print("ğŸ”„ [AIæœåŠ¡] RAGç®¡é“è°ƒç”¨LLMæœåŠ¡ï¼ˆç¦ç”¨çŸ¥è¯†åº“ä»¥é˜²é€’å½’ï¼‰")
        return get_answer_from_text(prompt, force_search=False, use_knowledge_base=False)
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] RAGçš„LLMè°ƒç”¨å¤±è´¥: {e}")
        logging.error(f"Error getting LLM response for RAG: {e}")
        return f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

def _extract_longest_json(text):
    """
    ä»æ–‡æœ¬ä¸­æå–æœ€é•¿çš„JSONç»“æ„ï¼Œä¼˜å…ˆåŒ¹é…[]æ•°ç»„ï¼Œç„¶ååŒ¹é…{}å¯¹è±¡

    Args:
        text (str): åŒ…å«JSONçš„æ–‡æœ¬

    Returns:
        str: æå–çš„JSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    def find_matching_bracket(text, start_pos, open_char, close_char):
        """æ‰¾åˆ°åŒ¹é…çš„æ‹¬å·ä½ç½®"""
        count = 1
        pos = start_pos + 1
        while pos < len(text) and count > 0:
            if text[pos] == open_char:
                count += 1
            elif text[pos] == close_char:
                count -= 1
            pos += 1
        return pos - 1 if count == 0 else -1

    # ä¼˜å…ˆæŸ¥æ‰¾æœ€é•¿çš„[]æ•°ç»„
    longest_array = ""
    for i, char in enumerate(text):
        if char == '[':
            end_pos = find_matching_bracket(text, i, '[', ']')
            if end_pos != -1:
                candidate = text[i:end_pos + 1]
                if len(candidate) > len(longest_array):
                    longest_array = candidate

    if longest_array:
        return longest_array

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°ç»„ï¼ŒæŸ¥æ‰¾æœ€é•¿çš„{}å¯¹è±¡
    longest_object = ""
    for i, char in enumerate(text):
        if char == '{':
            end_pos = find_matching_bracket(text, i, '{', '}')
            if end_pos != -1:
                candidate = text[i:end_pos + 1]
                if len(candidate) > len(longest_object):
                    longest_object = candidate

    return longest_object if longest_object else None

def get_direct_answer_from_image(image_bytes, force_search=False):
    """
    ç›´æ¥ä»å›¾åƒè·å–ç­”æ¡ˆï¼Œé€‚ç”¨äºåŒ…å«å›¾å½¢ã€å‡½æ•°å›¾ã€å‡ ä½•å›¾ç­‰è§†è§‰å…ƒç´ çš„é¢˜ç›®
    ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹ä¸€æ­¥åˆ°ä½è§£ç­”é—®é¢˜

    Args:
        image_bytes (bytes): The image data in bytes.
        force_search (bool): Whether to force the model to use search tools.

    Returns:
        str: The answer from the multimodal model, or an error message.
    """
    llm_config = get_model_config('llm')
    if not llm_config:
        return "Error: LLM configuration is missing or invalid."

    provider = llm_config.get('llm_provider', 'gemini')

    if provider == 'gemini':
        try:
            client_options = {
                "api_key": llm_config.get('google_api_key') or llm_config.get('api_key')
            }
            # æ³¨æ„ï¼šå¯¹äºåŸç”Ÿ Google GenAI APIï¼Œä¸éœ€è¦è®¾ç½® http_options
            client_options["http_options"] = {'base_url': llm_config.get('base_url',"https://generativelanguage.googleapis.com/v1beta/openai/")}

            genai_client = Client(**client_options)

            model_name = llm_config.get('model_name', 'gemini-2.5-flash')
            cur_time = time.strftime("%Y-%m-%d_%H-%M-%S")

            system_instruction = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”é¢˜åŠ©æ‰‹ã€‚ç°åœ¨æ—¶é—´æ˜¯{cur_time}ã€‚è¯·ç›´æ¥åˆ†æå›¾ç‰‡ä¸­çš„é—®é¢˜å¹¶ç»™å‡ºç­”æ¡ˆã€‚

å¯¹äºé€‰æ‹©é¢˜ï¼š
1. é¦–å…ˆç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š**ç­”æ¡ˆï¼šA** æˆ– **ç­”æ¡ˆï¼šAã€C**ï¼ˆå¤šé€‰é¢˜ï¼‰
2. ç„¶åç®€æ˜æ‰¼è¦è¯´æ˜ç†ç”±ï¼Œä¸è¦é•¿ç¯‡å¤§è®º

å¯¹äºä¸»è§‚é¢˜ï¼š
ç›´æ¥ç»™å‡ºå‡†ç¡®çš„ä¸­æ–‡ç­”æ¡ˆå’Œè§£é‡Š

ç‰¹åˆ«æ³¨æ„ï¼š
- ä»”ç»†è§‚å¯Ÿå›¾ç‰‡ä¸­çš„æ‰€æœ‰è§†è§‰å…ƒç´ ï¼ŒåŒ…æ‹¬å›¾å½¢ã€å›¾è¡¨ã€å‡½æ•°å›¾åƒã€å‡ ä½•å›¾å½¢ç­‰
- å¦‚æœé¢˜ç›®æ¶‰åŠå›¾å½¢åˆ†æï¼Œè¯·åŸºäºå›¾ç‰‡ä¸­çš„è§†è§‰ä¿¡æ¯è¿›è¡Œè§£ç­”
- å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”
- é€‰æ‹©é¢˜å¿…é¡»æ˜ç¡®æŒ‡å‡ºé€‰é¡¹å­—æ¯
- å›ç­”è¦å‡†ç¡®ã€ç®€æ´"""

            if force_search:
                system_instruction += " å¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·å¯»æ‰¾ç­”æ¡ˆ"

            # æ£€æµ‹å›¾ç‰‡æ ¼å¼
            def detect_image_mime_type(image_data):
                """æ£€æµ‹å›¾ç‰‡çš„MIMEç±»å‹"""
                if image_data.startswith(b'\x89PNG'):
                    return 'image/png'
                elif image_data.startswith(b'\xff\xd8\xff'):
                    return 'image/jpeg'
                elif image_data.startswith(b'GIF'):
                    return 'image/gif'
                elif image_data.startswith(b'RIFF') and b'WEBP' in image_data[:12]:
                    return 'image/webp'
                else:
                    return 'image/png'  # é»˜è®¤ä½¿ç”¨PNG

            mime_type = detect_image_mime_type(image_bytes)

            # ä½¿ç”¨æ­£ç¡®çš„ Google GenAI API æ ¼å¼æ„å»ºå†…å®¹
            content = [
                types.Part.from_bytes(
                    data=image_bytes,
                    mime_type=mime_type,
                ),
                "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„é—®é¢˜å¹¶ç»™å‡ºç­”æ¡ˆã€‚å¦‚æœå›¾ç‰‡åŒ…å«å›¾å½¢ã€å›¾è¡¨æˆ–å…¶ä»–è§†è§‰å…ƒç´ ï¼Œè¯·åŸºäºè¿™äº›è§†è§‰ä¿¡æ¯è¿›è¡Œåˆ†æã€‚"
            ]

            # æ„å»ºé…ç½®
            config_kwargs = {
                "system_instruction": system_instruction,
            }

            # åªæœ‰åœ¨éœ€è¦æœç´¢æ—¶æ‰æ·»åŠ å·¥å…·
            if force_search:
                config_kwargs["tools"] = [
                    types.Tool(
                        google_search=types.GoogleSearch()
                    )
                ]

            response = genai_client.models.generate_content(
                model=model_name,
                contents=content,
                config=types.GenerateContentConfig(**config_kwargs),
            )

            print(f"Response from Multimodal LLM: {response}")
            return response.text

        except Exception as e:
            return f"An error occurred while communicating with the Gemini API: {e}"

    else:
        # å¯¹äºéGeminiæä¾›å•†ï¼Œå›é€€åˆ°ä¸¤æ­¥èµ°æ–¹æ¡ˆ
        return "Error: Direct image answering is only supported with Gemini provider. Please use the two-step approach (extract question first, then answer)."

def get_question_from_image(image_bytes):
    """
    Sends an image to the configured VLM to extract a question.

    Args:
        image_bytes (bytes): The image data in bytes.

    Returns:
        str: The extracted question text, or an error message.
    """
    vlm_config = get_model_config('vlm')
    if not vlm_config:
        return "Error: VLM configuration is missing or invalid."

    # Encode the image in base64
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    # Set up httpx client with timeout and proxy if available
    client_kwargs = {"timeout": 30}
    if vlm_config.get('proxy'):
        client_kwargs['proxies'] = vlm_config['proxy']
    
    http_client = httpx.Client(**client_kwargs)

    chat = ChatOpenAI(
        model=vlm_config['model_name'],
        openai_api_key=vlm_config['api_key'],
        base_url=vlm_config['base_url'],
        http_client=http_client
    )

    try:
        response = chat.invoke(
            [
                SystemMessage(
                    content='''
ä»¥JSONæ•°ç»„æ ¼å¼æå–å›¾ä¸­æ‰€æœ‰é—®é¢˜ã€‚

æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«`question_text`(å­—ç¬¦ä¸²), `code_block`(å­—ç¬¦ä¸²æˆ–`null`), `options`(å­—ç¬¦ä¸²æ•°ç»„`[]`)ä»¥åŠ`question_type`å››ä¸ªé”®ã€‚
`question_type`çš„åˆ¤æ–­è§„åˆ™å¦‚ä¸‹ï¼š
- å¦‚æœé—®é¢˜æ–‡æœ¬åŒ…å«â€œå¤šé€‰â€ç­‰å…³é”®è¯ï¼Œæˆ–æ˜ç¡®æç¤ºå¯é€‰å¤šä¸ªç­”æ¡ˆï¼Œåˆ™ä¸º "å¤šé€‰é¢˜"ã€‚
- å¦‚æœ`options`æ•°ç»„ä¸ºç©ºï¼Œåˆ™ä¸º "ä¸»è§‚é¢˜"ã€‚
- å…¶ä»–æ‰€æœ‰æœ‰é€‰é¡¹çš„æƒ…å†µï¼Œé»˜è®¤ä¸º "å•é€‰é¢˜"ã€‚

å¯¹äºé€‰é¡¹æå–çš„é‡è¦è§„åˆ™ï¼š
1. å¿…é¡»åŒæ—¶åŒ…å«é€‰é¡¹æ ‡è¯†ï¼ˆå¦‚Aã€Bã€Cã€Dï¼‰å’Œé€‰é¡¹å†…å®¹
2. æ ¼å¼åº”ä¸ºï¼š"A. é€‰é¡¹å†…å®¹" æˆ– "Aã€é€‰é¡¹å†…å®¹" æˆ– "Aï¼šé€‰é¡¹å†…å®¹" ç­‰
3. å³ä½¿åŸé¢˜ä¸­é€‰é¡¹æ ‡è¯†å’Œå†…å®¹æ˜¯åˆ†ç¦»çš„ï¼Œä¹Ÿå¿…é¡»å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´é€‰é¡¹


ç¤ºä¾‹:
```json
[
  {
    "question_text": "é¢˜ç›®çš„ä¸»è¦æ–‡æœ¬",
    "code_block": "å®Œæ•´çš„ä»£ç å†…å®¹",
    "options": ["A. ç¬¬ä¸€ä¸ªé€‰é¡¹å†…å®¹", "B. ç¬¬äºŒä¸ªé€‰é¡¹å†…å®¹", "C. ç¬¬ä¸‰ä¸ªé€‰é¡¹å†…å®¹", "D. ç¬¬å››ä¸ªé€‰é¡¹å†…å®¹"],
    "question_type": "å•é€‰é¢˜"
  }
]
````

ä½ çš„å›ç­”å¿…é¡»åªåŒ…å«JSONæ•°ç»„ï¼Œæ— ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚è‹¥å›¾ä¸­æ— é—®é¢˜ï¼Œåˆ™è¿”å›ç©ºæ•°ç»„`[]`ã€‚
                    '''
                ),
                HumanMessage(
                    content=[
                        {"type": "text", "text": "Extract the question from this image."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64_image}"
                            },
                        },
                    ]
                ),
            ]
        )
        print(f"Response from VLM: {response}")

        # éªŒè¯è¿”å›çš„å†…å®¹æ˜¯å¦ä¸ºæœ‰æ•ˆçš„JSON
        response_content = response.content.strip()

        # å…ˆå°è¯•æå–æœ€é•¿çš„JSONç»“æ„
        json_content = _extract_longest_json(response_content)
        if not json_content:
            return f"Error: No valid JSON structure found in VLM response: {response_content}"

        try:
            # å°è¯•è§£ææå–çš„JSON
            parsed_json = json.loads(json_content)

            # å¦‚æœæ˜¯ç©ºæ•°ç»„ï¼Œç›´æ¥è¿”å›
            if isinstance(parsed_json, list) and len(parsed_json) == 0:
                return "[]"

            # å¦‚æœè§£ææˆåŠŸä¸”ä¸ä¸ºç©ºï¼Œè¿”å›æå–çš„JSONå†…å®¹
            return json_content

        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return f"Error: VLM returned invalid JSON format: {response_content}"

    except Exception as e:
        return f"An error occurred while communicating with the VLM: {e}"


def get_answer_from_text(question_text, force_search=False, use_knowledge_base=True):
    """
    Sends a question text to the configured LLM to get an answer.
    Optionally uses knowledge base for enhanced responses.

    Args:
        question_text (str): The question to be answered.
        force_search (bool): Whether to force search tools usage.
        use_knowledge_base (bool): Whether to use knowledge base for enhanced answers.

    Returns:
        str: The answer from the LLM, or an error message.
    """
    # Try to use knowledge base first if available and enabled
    print(f"ğŸ¤– [AIæœåŠ¡] get_answer_from_text è°ƒç”¨ï¼Œuse_knowledge_base={use_knowledge_base}")
    if use_knowledge_base and is_knowledge_base_available():
        print("ğŸš€ [AIæœåŠ¡] çŸ¥è¯†åº“å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨RAGå¢å¼ºå“åº”")
        try:
            rag_pipeline = get_rag_pipeline()
            if rag_pipeline and rag_pipeline.should_use_knowledge_base():
                print("âœ… [AIæœåŠ¡] RAGç®¡é“å¯ç”¨ä¸”åº”è¯¥ä½¿ç”¨çŸ¥è¯†åº“")
                print("ğŸ” [AIæœåŠ¡] è°ƒç”¨RAGç®¡é“å¤„ç†æŸ¥è¯¢...")
                logging.info("Using knowledge base for enhanced response")
                enhanced_response = rag_pipeline.process_query_with_knowledge(question_text)
                if enhanced_response and not enhanced_response.startswith("æŠ±æ­‰"):
                    print("âœ… [AIæœåŠ¡] RAGå¢å¼ºå“åº”æˆåŠŸï¼Œè¿”å›ç»“æœ")
                    print(f"ğŸ“ [AIæœåŠ¡] å“åº”é•¿åº¦: {len(enhanced_response)} å­—ç¬¦")
                    return enhanced_response
                else:
                    print("âš ï¸ [AIæœåŠ¡] RAGå“åº”ä¸æ»¡æ„ï¼Œå›é€€åˆ°æ ‡å‡†LLM")
                    logging.info("Knowledge base response not satisfactory, falling back to standard LLM")
            else:
                print("âŒ [AIæœåŠ¡] RAGç®¡é“ä¸å¯ç”¨æˆ–ä¸åº”ä½¿ç”¨çŸ¥è¯†åº“")
        except Exception as e:
            print(f"âŒ [AIæœåŠ¡] ä½¿ç”¨çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            logging.error(f"Error using knowledge base: {e}")
            logging.info("Falling back to standard LLM response")
    else:
        if not use_knowledge_base:
            print("ğŸ”’ [AIæœåŠ¡] çŸ¥è¯†åº“ä½¿ç”¨è¢«ç¦ç”¨")
        else:
            print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨")
    
    # Standard LLM processing (original implementation)
    llm_config = get_model_config('llm')
    if not llm_config:
        return "Error: LLM configuration is missing or invalid."

    provider = llm_config.get('llm_provider', 'gemini')
    chat = None

    if provider == 'gemini':
        try:
            client_options = {
                "api_key": llm_config.get('google_api_key') or llm_config.get('api_key')
            }
            client_options["http_options"] = {'base_url': llm_config.get('base_url',"https://generativelanguage.googleapis.com/v1beta/openai/")}

            genai_client = Client(**client_options)
            
            model_name = llm_config.get('model_name', 'gemini-2.5-flash')
            # model = genai.GenerativeModel(model_name)
            cur_time = time.strftime("%Y-%m-%d_%H-%M-%S")
            system_instruction = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”é¢˜åŠ©æ‰‹ã€‚ç°åœ¨æ—¶é—´æ˜¯{cur_time}ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”é—®é¢˜ï¼š
            å¯¹äºé€‰æ‹©é¢˜ï¼š
            1. é¦–å…ˆç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š**ç­”æ¡ˆï¼šA** æˆ– **ç­”æ¡ˆï¼šAã€C**ï¼ˆå¤šé€‰é¢˜ï¼‰
            2. ç„¶åç®€æ˜æ‰¼è¦è¯´æ˜ç†ç”±ï¼Œä¸è¦é•¿ç¯‡å¤§è®º

            å¯¹äºä¸»è§‚é¢˜ï¼š
            ç›´æ¥ç»™å‡ºå‡†ç¡®çš„ä¸­æ–‡ç­”æ¡ˆå’Œè§£é‡Š

            è¦æ±‚ï¼š
            - å¿…é¡»ä½¿ç”¨ä¸­æ–‡å›ç­”
            - é€‰æ‹©é¢˜å¿…é¡»æ˜ç¡®æŒ‡å‡ºé€‰é¡¹å­—æ¯
            - å›ç­”è¦å‡†ç¡®ã€ç®€æ´
            - æ³¨æ„é¢˜ç›®å¯èƒ½æ¥è‡ªOCRï¼Œå­˜åœ¨è¯†åˆ«é”™è¯¯ï¼Œè¯·åˆç†åˆ¤æ–­"""
            if force_search:
                system_instruction += " å¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·å¯»æ‰¾ç­”æ¡ˆ"
            content = question_text
            response = genai_client.models.generate_content(
                model=model_name,
                contents=content,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    tools=[
                        types.Tool(
                            google_search=types.GoogleSearch()
                        )
                    ],
                    
                ),
            )
            
            print(f"Response from LLM: {response}")
            return response.text
        except Exception as e:
            return f"An error occurred while communicating with the Gemini API: {e}"

    else:
        # For other providers, use the existing ChatOpenAI setup.
        # Set up httpx client with timeout and proxy if available
        client_kwargs = {"timeout": 30}
        if llm_config.get('proxy'):
            client_kwargs['proxies'] = llm_config['proxy']
            
        http_client = httpx.Client(**client_kwargs)

        chat = ChatOpenAI(
            model=llm_config['model_name'],
            openai_api_key=llm_config['api_key'],
            base_url=llm_config['base_url'],
            http_client=http_client
        )
        try:
            system_prompt = "You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡åŠ©æ‰‹ã€‚æ— è®ºç”¨æˆ·æé—®ä½¿ç”¨ä»€ä¹ˆè¯­è¨€ï¼Œä½ éƒ½å¿…é¡»å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›ç­”æ‰€æœ‰é—®é¢˜ã€‚å¯¹äºé€‰æ‹©é¢˜ï¼ˆå•é€‰æˆ–å¤šé€‰ï¼‰ï¼Œè¯·å…ˆåˆ†æé—®é¢˜ï¼Œç„¶åæ˜ç¡®æŒ‡å‡ºæ­£ç¡®ç­”æ¡ˆçš„é€‰é¡¹ï¼ˆå¦‚'ç­”æ¡ˆæ˜¯B'æˆ–'ç­”æ¡ˆæ˜¯Aå’ŒC'ï¼‰ã€‚ä¹‹åå†æä¾›è¯¦ç»†è§£é‡Šã€‚Provide a concise and accurate answer to the user's question. å³ä½¿é¢˜ç›®å…¨æ˜¯è‹±æ–‡ï¼Œä½ ä¹Ÿå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ã€‚"
            if force_search:
                logging.warning("OpenAI compatibility mode: force_search is not usable")
                # system_prompt += " å¿…é¡»ä½¿ç”¨æœç´¢å·¥å…·å¯»æ‰¾ç­”æ¡ˆ"
            response = chat.invoke(
                [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=question_text),
                ]
            )
            print(f"Response from LLM: {response}")
            return response.content
        except Exception as e:
            return f"An error occurred while communicating with the LLM: {e}"

# Knowledge Base Management Functions

def get_knowledge_base_status():
    """
    Get the current status of the knowledge base.
    
    Returns:
        dict: Knowledge base status information
    """
    if not is_knowledge_base_available():
        return {
            "available": False,
            "enabled": False,
            "error": "Knowledge base not initialized or not available"
        }
    
    try:
        rag_pipeline = get_rag_pipeline()
        return rag_pipeline.get_knowledge_base_status()
    except Exception as e:
        return {
            "available": False,
            "enabled": False,
            "error": str(e)
        }

def set_knowledge_base_collections(collection_ids):
    """
    Set the collections to use for knowledge base queries.
    
    Args:
        collection_ids (list): List of collection IDs to use
        
    Returns:
        bool: True if successful, False otherwise
    """
    if not is_knowledge_base_available():
        return False
    
    try:
        rag_pipeline = get_rag_pipeline()
        rag_pipeline.set_selected_collections(collection_ids)
        return True
    except Exception as e:
        logging.error(f"Error setting knowledge base collections: {e}")
        return False

def get_knowledge_base_collections():
    """
    Get available knowledge base collections.
    
    Returns:
        list: List of available collections
    """
    if not is_knowledge_base_available():
        return []
    
    try:
        kb_manager = get_knowledge_base_manager()
        return kb_manager.list_collections()
    except Exception as e:
        logging.error(f"Error getting knowledge base collections: {e}")
        return []

def enable_knowledge_base():
    """
    Enable knowledge base functionality.
    
    Returns:
        bool: True if successful, False otherwise
    """
    print("ğŸš€ [AIæœåŠ¡] å¯ç”¨çŸ¥è¯†åº“åŠŸèƒ½...")
    
    if not is_knowledge_base_available():
        print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨ï¼Œæ— æ³•å¯ç”¨")
        return False
    
    try:
        print("ğŸ”§ [AIæœåŠ¡] è·å–RAGç®¡é“å®ä¾‹...")
        rag_pipeline = get_rag_pipeline()
        
        print("âš¡ [AIæœåŠ¡] è°ƒç”¨RAGç®¡é“çš„å¯ç”¨çŸ¥è¯†åº“æ–¹æ³•...")
        rag_pipeline.enable_knowledge_base()
        
        print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“åŠŸèƒ½å¯ç”¨æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] å¯ç”¨çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(f"ğŸ” [AIæœåŠ¡] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logging.error(f"Error enabling knowledge base: {e}")
        return False

def disable_knowledge_base():
    """
    Disable knowledge base functionality.
    
    Returns:
        bool: True if successful, False otherwise
    """
    print("ğŸ›‘ [AIæœåŠ¡] ç¦ç”¨çŸ¥è¯†åº“åŠŸèƒ½...")
    
    if not is_knowledge_base_available():
        print("âŒ [AIæœåŠ¡] çŸ¥è¯†åº“ä¸å¯ç”¨ï¼Œæ— æ³•ç¦ç”¨")
        return False
    
    try:
        print("ğŸ”§ [AIæœåŠ¡] è·å–RAGç®¡é“å®ä¾‹...")
        rag_pipeline = get_rag_pipeline()
        
        print("âš¡ [AIæœåŠ¡] è°ƒç”¨RAGç®¡é“çš„ç¦ç”¨çŸ¥è¯†åº“æ–¹æ³•...")
        rag_pipeline.disable_knowledge_base()
        
        print("âœ… [AIæœåŠ¡] çŸ¥è¯†åº“åŠŸèƒ½ç¦ç”¨æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ [AIæœåŠ¡] ç¦ç”¨çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        print(f"ğŸ” [AIæœåŠ¡] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        logging.error(f"Error disabling knowledge base: {e}")
        return False

def search_knowledge_preview(query, collections=None, top_k=3):
    """
    Search knowledge base and return preview results.
    
    Args:
        query (str): Search query
        collections (list): Collections to search (None for all selected)
        top_k (int): Maximum number of results
        
    Returns:
        list: List of knowledge fragment previews
    """
    if not is_knowledge_base_available():
        return []
    
    try:
        rag_pipeline = get_rag_pipeline()
        return rag_pipeline.search_knowledge_preview(query, collections, top_k)
    except Exception as e:
        logging.error(f"Error searching knowledge base: {e}")
        return []

def get_knowledge_base_statistics():
    """
    Get knowledge base statistics.
    
    Returns:
        dict: Knowledge base statistics
    """
    if not is_knowledge_base_available():
        return {"error": "Knowledge base not available"}
    
    try:
        kb_manager = get_knowledge_base_manager()
        return kb_manager.get_knowledge_base_stats()
    except Exception as e:
        logging.error(f"Error getting knowledge base statistics: {e}")
        return {"error": str(e)}

# Initialize knowledge base on module import
try:
    initialize_knowledge_base()
except Exception as e:
    logging.error(f"Failed to initialize knowledge base on module import: {e}")