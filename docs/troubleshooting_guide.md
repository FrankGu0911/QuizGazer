# çŸ¥è¯†åº“RAGç³»ç»Ÿæ•…éšœæ’é™¤æŒ‡å—

æœ¬æŒ‡å—æä¾›äº†çŸ¥è¯†åº“RAGç³»ç»Ÿå¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

## ç›®å½•

- [å®‰è£…å’Œé…ç½®é—®é¢˜](#å®‰è£…å’Œé…ç½®é—®é¢˜)
- [æ–‡æ¡£å¤„ç†é—®é¢˜](#æ–‡æ¡£å¤„ç†é—®é¢˜)
- [æœç´¢å’Œæ£€ç´¢é—®é¢˜](#æœç´¢å’Œæ£€ç´¢é—®é¢˜)
- [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
- [å†…å­˜å’Œèµ„æºé—®é¢˜](#å†…å­˜å’Œèµ„æºé—®é¢˜)
- [APIå’ŒæœåŠ¡é—®é¢˜](#apiå’ŒæœåŠ¡é—®é¢˜)
- [æ•°æ®åº“è¿æ¥é—®é¢˜](#æ•°æ®åº“è¿æ¥é—®é¢˜)
- [UIç•Œé¢é—®é¢˜](#uiç•Œé¢é—®é¢˜)
- [æ—¥å¿—å’Œè°ƒè¯•](#æ—¥å¿—å’Œè°ƒè¯•)

## å®‰è£…å’Œé…ç½®é—®é¢˜

### é—®é¢˜1ï¼šä¾èµ–åŒ…å®‰è£…å¤±è´¥

**ç—‡çŠ¶**ï¼š
```bash
ERROR: Could not find a version that satisfies the requirement chromadb
```

**å¯èƒ½åŸå› **ï¼š
- Pythonç‰ˆæœ¬ä¸å…¼å®¹
- ç½‘ç»œè¿æ¥é—®é¢˜
- pipç‰ˆæœ¬è¿‡æ—§

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆéœ€è¦3.8+ï¼‰
python --version

# å‡çº§pip
pip install --upgrade pip

# ä½¿ç”¨å›½å†…é•œåƒæº
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/

# å¦‚æœä»æœ‰é—®é¢˜ï¼Œå°è¯•å•ç‹¬å®‰è£…é—®é¢˜åŒ…
pip install chromadb --no-cache-dir
```

### é—®é¢˜2ï¼šé…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥

**ç—‡çŠ¶**ï¼š
```python
FileNotFoundError: [Errno 2] No such file or directory: 'config/knowledge_base.json'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åˆ›å»ºé»˜è®¤é…ç½®
from utils.config_manager import save_knowledge_base_config

default_config = {
    "enabled": True,
    "storage_path": "./data/knowledge_base",
    "max_file_size_mb": 100,
    "chunk_size": 1000,
    "chunk_overlap": 200
}

save_knowledge_base_config(default_config)
```

### é—®é¢˜3ï¼šç¯å¢ƒå˜é‡æœªè®¾ç½®

**ç—‡çŠ¶**ï¼š
```python
ValueError: API key not found
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# åˆ›å»º.envæ–‡ä»¶
echo "OPENAI_API_KEY=your-api-key-here" > .env
echo "CHROMADB_PATH=./data/chromadb" >> .env

# æˆ–è€…åœ¨ä»£ç ä¸­è®¾ç½®
import os
os.environ['OPENAI_API_KEY'] = 'your-api-key-here'
```

## æ–‡æ¡£å¤„ç†é—®é¢˜

### é—®é¢˜4ï¼šæ–‡æ¡£ä¸Šä¼ å¤±è´¥

**ç—‡çŠ¶**ï¼š
```python
KnowledgeBaseError: Failed to process document: unsupported file format
```

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
# æ£€æŸ¥æ–‡ä»¶æ ¼å¼
import os
file_path = "your_document.pdf"
file_extension = os.path.splitext(file_path)[1].lower()
print(f"æ–‡ä»¶æ‰©å±•å: {file_extension}")

# æ£€æŸ¥æ–‡ä»¶å¤§å°
file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
print(f"æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è¯»
try:
    with open(file_path, 'rb') as f:
        f.read(100)
    print("æ–‡ä»¶å¯è¯»")
except Exception as e:
    print(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç¡®è®¤æ–‡ä»¶æ ¼å¼åœ¨æ”¯æŒåˆ—è¡¨ä¸­ï¼ˆ.md, .txt, .csv, .pdf, .docxï¼‰
- æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…è¿‡é™åˆ¶
- ç¡®è®¤æ–‡ä»¶æœªæŸåä¸”å¯è¯»
- æ£€æŸ¥æ–‡ä»¶ç¼–ç ï¼ˆå»ºè®®ä½¿ç”¨UTF-8ï¼‰

### é—®é¢˜5ï¼šPDFæ–‡æ¡£å¤„ç†å¤±è´¥

**ç—‡çŠ¶**ï¼š
```python
ProcessingError: Failed to extract text from PDF
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# å®‰è£…é¢å¤–çš„PDFå¤„ç†ä¾èµ–
pip install PyPDF2 pdfplumber

# æ£€æŸ¥PDFæ–‡ä»¶
import PyPDF2
try:
    with open('document.pdf', 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        print(f"PDFé¡µæ•°: {len(reader.pages)}")
        print(f"ç¬¬ä¸€é¡µæ–‡æœ¬: {reader.pages[0].extract_text()[:100]}")
except Exception as e:
    print(f"PDFå¤„ç†é”™è¯¯: {e}")
```

### é—®é¢˜6ï¼šæ–‡æ¡£å¤„ç†è¶…æ—¶

**ç—‡çŠ¶**ï¼š
```python
TimeoutError: Document processing timeout after 300 seconds
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# è°ƒæ•´è¶…æ—¶è®¾ç½®
kb_config = {
    "processing_timeout": 600,  # å¢åŠ åˆ°10åˆ†é’Ÿ
    "max_concurrent_tasks": 2   # å‡å°‘å¹¶å‘ä»»åŠ¡
}

# åˆ†æ‰¹å¤„ç†å¤§æ–‡æ¡£
def process_large_document(file_path, collection_id):
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
    
    if file_size_mb > 50:  # å¤§äº50MB
        print(f"å¤§æ–‡æ¡£ ({file_size_mb:.1f}MB)ï¼Œå»ºè®®åˆ†å‰²åå¤„ç†")
        # å®ç°æ–‡æ¡£åˆ†å‰²é€»è¾‘
    else:
        return kb_manager.add_document_async(collection_id, file_path, doc_type)
```

## æœç´¢å’Œæ£€ç´¢é—®é¢˜

### é—®é¢˜7ï¼šæœç´¢ç»“æœä¸å‡†ç¡®

**ç—‡çŠ¶**ï¼šæœç´¢è¿”å›ä¸ç›¸å…³çš„ç»“æœ

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
# æ£€æŸ¥æœç´¢å‚æ•°
query = "ä½ çš„æŸ¥è¯¢"
results = kb_manager.search_knowledge(
    query=query,
    collection_ids=[collection_id],
    top_k=10  # å¢åŠ ç»“æœæ•°é‡è¿›è¡Œåˆ†æ
)

# åˆ†æç»“æœç›¸å…³æ€§
for i, result in enumerate(results):
    print(f"ç»“æœ {i+1}:")
    print(f"  ç›¸å…³åº¦: {result.relevance_score:.3f}")
    print(f"  å†…å®¹: {result.content[:200]}...")
    print(f"  æ¥æº: {result.source_document}")
    print()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. ä¼˜åŒ–æŸ¥è¯¢è¯
def optimize_query(original_query):
    # æ·»åŠ åŒä¹‰è¯æˆ–ç›¸å…³è¯
    synonyms = {
        "API": ["æ¥å£", "åº”ç”¨ç¨‹åºæ¥å£"],
        "é…ç½®": ["è®¾ç½®", "é…ç½®æ–‡ä»¶", "å‚æ•°"]
    }
    
    optimized_query = original_query
    for key, values in synonyms.items():
        if key in original_query:
            optimized_query += " " + " ".join(values)
    
    return optimized_query

# 2. è°ƒæ•´åˆ†å—å‚æ•°
kb_config = {
    "chunk_size": 800,      # å‡å°åˆ†å—å¤§å°
    "chunk_overlap": 150    # å¢åŠ é‡å 
}

# 3. ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢ç­–ç•¥
def multi_query_search(query, collection_ids, top_k=5):
    queries = [
        query,
        f"å…³äº{query}çš„ä¿¡æ¯",
        f"{query}æ˜¯ä»€ä¹ˆ",
        f"å¦‚ä½•{query}"
    ]
    
    all_results = []
    for q in queries:
        results = kb_manager.search_knowledge(q, collection_ids, top_k)
        all_results.extend(results)
    
    # å»é‡å¹¶æŒ‰ç›¸å…³æ€§æ’åº
    unique_results = {}
    for result in all_results:
        key = result.content[:100]  # ä½¿ç”¨å†…å®¹å‰100å­—ç¬¦ä½œä¸ºå»é‡é”®
        if key not in unique_results or result.relevance_score > unique_results[key].relevance_score:
            unique_results[key] = result
    
    return sorted(unique_results.values(), key=lambda x: x.relevance_score, reverse=True)[:top_k]
```

### é—®é¢˜8ï¼šæœç´¢é€Ÿåº¦æ…¢

**ç—‡çŠ¶**ï¼šæœç´¢å“åº”æ—¶é—´è¶…è¿‡5ç§’

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
import time

# æµ‹é‡æœç´¢æ—¶é—´
start_time = time.time()
results = kb_manager.search_knowledge("æµ‹è¯•æŸ¥è¯¢", [collection_id], top_k=5)
search_time = time.time() - start_time

print(f"æœç´¢è€—æ—¶: {search_time:.2f}ç§’")
print(f"ç»“æœæ•°é‡: {len(results)}")

# æ£€æŸ¥é›†åˆå¤§å°
collection = kb_manager.get_collection(collection_id)
print(f"é›†åˆæ–‡æ¡£æ•°: {collection.document_count}")
print(f"æ€»å—æ•°: {collection.total_chunks}")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å¯ç”¨ç¼“å­˜
from core.knowledge_base.optimized_manager import create_optimized_manager

cache_config = {
    'query_cache': {
        'max_size': 1000,
        'max_memory_mb': 100,
        'default_ttl': 3600
    }
}

optimized_manager = create_optimized_manager(cache_config=cache_config)

# 2. å‡å°‘æœç´¢èŒƒå›´
# åªæœç´¢ç›¸å…³é›†åˆ
relevant_collections = filter_relevant_collections(query)
results = optimized_manager.search_knowledge(
    query, 
    collection_ids=relevant_collections, 
    top_k=5
)

# 3. ä½¿ç”¨æ‰¹é‡æœç´¢
queries = ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"]
bulk_results = optimized_manager.bulk_search(queries, [collection_id])
```

## æ€§èƒ½é—®é¢˜

### é—®é¢˜9ï¼šç³»ç»Ÿå“åº”ç¼“æ…¢

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
# è·å–æ€§èƒ½æ´å¯Ÿ
from core.knowledge_base.optimized_manager import create_optimized_manager

optimized_manager = create_optimized_manager()
insights = optimized_manager.get_performance_insights()

print("æ€§èƒ½é—®é¢˜:")
for issue in insights['performance_issues']:
    print(f"- {issue['type']}: {issue.get('severity', 'unknown')}")

print("\nä¼˜åŒ–å»ºè®®:")
for suggestion in insights['optimization_suggestions']:
    print(f"- {suggestion['type']}: {suggestion['reason']}")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å¯ç”¨æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½
cache_config = {
    'embedding_cache': {
        'max_size': 10000,
        'max_memory_mb': 500
    },
    'query_cache': {
        'max_size': 1000,
        'max_memory_mb': 100
    },
    'connection_pool': {
        'max_connections': 10
    }
}

optimized_manager = create_optimized_manager(cache_config=cache_config)

# 2. å®šæœŸå†…å­˜ä¼˜åŒ–
import threading
import time

def memory_optimization_task():
    while True:
        time.sleep(300)  # æ¯5åˆ†é’Ÿ
        optimized_manager.optimize_memory_usage()

optimization_thread = threading.Thread(target=memory_optimization_task, daemon=True)
optimization_thread.start()

# 3. è°ƒæ•´å¹¶å‘å‚æ•°
kb_config = {
    "max_concurrent_tasks": 2,  # å‡å°‘å¹¶å‘ä»»åŠ¡
    "batch_size": 5            # å‡å°æ‰¹å¤„ç†å¤§å°
}
```

### é—®é¢˜10ï¼šé«˜CPUä½¿ç”¨ç‡

**ç—‡çŠ¶**ï¼šCPUä½¿ç”¨ç‡æŒç»­è¶…è¿‡80%

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
import psutil
import time

# ç›‘æ§CPUä½¿ç”¨ç‡
def monitor_cpu(duration=60):
    cpu_usage = []
    for _ in range(duration):
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_usage.append(cpu_percent)
        print(f"CPUä½¿ç”¨ç‡: {cpu_percent}%")
    
    avg_cpu = sum(cpu_usage) / len(cpu_usage)
    print(f"å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
    return avg_cpu

# æ£€æŸ¥è¿›ç¨‹CPUä½¿ç”¨
process = psutil.Process()
print(f"å½“å‰è¿›ç¨‹CPUä½¿ç”¨ç‡: {process.cpu_percent()}%")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. é™åˆ¶å¹¶å‘å¤„ç†
kb_config = {
    "max_concurrent_tasks": 1,
    "max_workers": 2
}

# 2. ä½¿ç”¨æ›´é«˜æ•ˆçš„æ¨¡å‹
# é€‰æ‹©è®¡ç®—é‡è¾ƒå°çš„åµŒå…¥æ¨¡å‹
embedding_config = {
    "model": "text-embedding-ada-002",  # ç›¸å¯¹è½»é‡
    "batch_size": 16  # å‡å°æ‰¹å¤„ç†å¤§å°
}

# 3. å®ç°CPUä½¿ç”¨ç‡é™åˆ¶
import time
import threading

class CPUThrottler:
    def __init__(self, max_cpu_percent=70):
        self.max_cpu_percent = max_cpu_percent
        self.monitoring = True
    
    def start_monitoring(self):
        def monitor():
            while self.monitoring:
                cpu_percent = psutil.cpu_percent(interval=1)
                if cpu_percent > self.max_cpu_percent:
                    print(f"CPUä½¿ç”¨ç‡è¿‡é«˜ ({cpu_percent}%)ï¼Œæš‚åœå¤„ç†...")
                    time.sleep(2)  # æš‚åœ2ç§’
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()

throttler = CPUThrottler()
throttler.start_monitoring()
```

## å†…å­˜å’Œèµ„æºé—®é¢˜

### é—®é¢˜11ï¼šå†…å­˜ä¸è¶³é”™è¯¯

**ç—‡çŠ¶**ï¼š
```python
MemoryError: Unable to allocate memory
```

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
import psutil

# æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
def check_memory_usage():
    # ç³»ç»Ÿå†…å­˜
    memory = psutil.virtual_memory()
    print(f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡: {memory.percent}%")
    print(f"å¯ç”¨å†…å­˜: {memory.available / 1024 / 1024 / 1024:.1f} GB")
    
    # è¿›ç¨‹å†…å­˜
    process = psutil.Process()
    process_memory = process.memory_info()
    print(f"è¿›ç¨‹å†…å­˜ä½¿ç”¨: {process_memory.rss / 1024 / 1024:.1f} MB")
    
    # ç¼“å­˜å†…å­˜ä½¿ç”¨
    if hasattr(optimized_manager, 'cache_manager'):
        cache_stats = optimized_manager.get_cache_statistics()
        print(f"ç¼“å­˜å†…å­˜ä½¿ç”¨: {cache_stats.get('total_memory_mb', 0):.1f} MB")

check_memory_usage()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å‡å°‘ç¼“å­˜å¤§å°
cache_config = {
    'embedding_cache': {
        'max_size': 5000,      # å‡å°‘åˆ°5000
        'max_memory_mb': 200   # å‡å°‘åˆ°200MB
    },
    'query_cache': {
        'max_size': 500,       # å‡å°‘åˆ°500
        'max_memory_mb': 50    # å‡å°‘åˆ°50MB
    }
}

# 2. å®ç°å†…å­˜ç›‘æ§å’Œè‡ªåŠ¨æ¸…ç†
class MemoryManager:
    def __init__(self, max_memory_mb=1000):
        self.max_memory_mb = max_memory_mb
    
    def check_and_cleanup(self):
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > self.max_memory_mb:
            print(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({memory_mb:.1f}MB)ï¼Œå¼€å§‹æ¸…ç†...")
            
            # æ¸…ç†ç¼“å­˜
            optimized_manager.cache_manager.clear_all_caches()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # å†æ¬¡æ£€æŸ¥
            new_memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
            print(f"æ¸…ç†åå†…å­˜ä½¿ç”¨: {new_memory_mb:.1f}MB")

memory_manager = MemoryManager(max_memory_mb=800)

# 3. åˆ†æ‰¹å¤„ç†å¤§é‡æ•°æ®
def process_documents_with_memory_limit(documents, collection_id):
    memory_manager = MemoryManager()
    
    for i, (file_path, doc_type) in enumerate(documents):
        # æ¯å¤„ç†5ä¸ªæ–‡æ¡£æ£€æŸ¥ä¸€æ¬¡å†…å­˜
        if i % 5 == 0:
            memory_manager.check_and_cleanup()
        
        try:
            task = kb_manager.add_document_async(collection_id, file_path, doc_type)
            print(f"å·²æäº¤æ–‡æ¡£ {i+1}/{len(documents)}: {file_path}")
        except MemoryError:
            print("å†…å­˜ä¸è¶³ï¼Œæš‚åœå¤„ç†...")
            memory_manager.check_and_cleanup()
            time.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
```

### é—®é¢˜12ï¼šç£ç›˜ç©ºé—´ä¸è¶³

**ç—‡çŠ¶**ï¼š
```python
OSError: [Errno 28] No space left on device
```

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
import shutil

# æ£€æŸ¥ç£ç›˜ç©ºé—´
def check_disk_space(path="."):
    total, used, free = shutil.disk_usage(path)
    
    print(f"ç£ç›˜æ€»ç©ºé—´: {total / 1024 / 1024 / 1024:.1f} GB")
    print(f"å·²ä½¿ç”¨ç©ºé—´: {used / 1024 / 1024 / 1024:.1f} GB")
    print(f"å¯ç”¨ç©ºé—´: {free / 1024 / 1024 / 1024:.1f} GB")
    print(f"ä½¿ç”¨ç‡: {(used / total) * 100:.1f}%")
    
    return free

free_space = check_disk_space()
if free_space < 1024 * 1024 * 1024:  # å°äº1GB
    print("âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ï¼")
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜
import os
import glob

def cleanup_temp_files():
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    temp_patterns = [
        "./temp/*",
        "./data/temp/*",
        "./__pycache__/*",
        "./logs/*.log.old"
    ]
    
    for pattern in temp_patterns:
        for file_path in glob.glob(pattern):
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

# 2. å‹ç¼©æ—§æ•°æ®
def compress_old_data():
    import zipfile
    import datetime
    
    # å‹ç¼©30å¤©å‰çš„æ—¥å¿—æ–‡ä»¶
    cutoff_date = datetime.datetime.now() - datetime.timedelta(days=30)
    
    for log_file in glob.glob("./logs/*.log"):
        file_time = datetime.datetime.fromtimestamp(os.path.getmtime(log_file))
        if file_time < cutoff_date:
            zip_name = f"{log_file}.zip"
            with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                zipf.write(log_file, os.path.basename(log_file))
            os.remove(log_file)
            print(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶: {log_file} -> {zip_name}")

# 3. å®ç°ç£ç›˜ç©ºé—´ç›‘æ§
class DiskSpaceMonitor:
    def __init__(self, min_free_gb=2):
        self.min_free_gb = min_free_gb
    
    def check_space(self, path="."):
        _, _, free = shutil.disk_usage(path)
        free_gb = free / 1024 / 1024 / 1024
        
        if free_gb < self.min_free_gb:
            print(f"âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³: {free_gb:.1f}GB < {self.min_free_gb}GB")
            self.cleanup()
            return False
        return True
    
    def cleanup(self):
        cleanup_temp_files()
        compress_old_data()
        
        # æ¸…ç†ç¼“å­˜
        optimized_manager.clear_all_caches()

disk_monitor = DiskSpaceMonitor()
```

## APIå’ŒæœåŠ¡é—®é¢˜

### é—®é¢˜13ï¼šAPIè°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**ï¼š
```python
APIError: Rate limit exceeded
```

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
# æ£€æŸ¥APIé…ç½®
def diagnose_api_config():
    import os
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ APIå¯†é’¥æœªè®¾ç½®")
        return False
    
    if not api_key.startswith('sk-'):
        print("âŒ APIå¯†é’¥æ ¼å¼ä¸æ­£ç¡®")
        return False
    
    print("âœ… APIå¯†é’¥æ ¼å¼æ­£ç¡®")
    
    # æµ‹è¯•APIè¿æ¥
    try:
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„APIæµ‹è¯•
        print("âœ… APIè¿æ¥æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

diagnose_api_config()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å®ç°é‡è¯•æœºåˆ¶
import time
import random
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    
                    # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    print(f"APIè°ƒç”¨å¤±è´¥ï¼Œ{delay:.1f}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(delay)
            
            return None
        return wrapper
    return decorator

# 2. å®ç°é€Ÿç‡é™åˆ¶
class RateLimiter:
    def __init__(self, max_requests_per_minute=60):
        self.max_requests = max_requests_per_minute
        self.requests = []
    
    def wait_if_needed(self):
        now = time.time()
        
        # æ¸…ç†1åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
        self.requests = [req_time for req_time in self.requests if now - req_time < 60]
        
        if len(self.requests) >= self.max_requests:
            sleep_time = 60 - (now - self.requests[0])
            if sleep_time > 0:
                print(f"è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_time:.1f} ç§’...")
                time.sleep(sleep_time)
        
        self.requests.append(now)

rate_limiter = RateLimiter(max_requests_per_minute=50)

@retry_with_backoff(max_retries=3)
def api_call_with_rate_limit(func, *args, **kwargs):
    rate_limiter.wait_if_needed()
    return func(*args, **kwargs)

# 3. é…ç½®å¤šä¸ªAPIå¯†é’¥è½®æ¢
class APIKeyRotator:
    def __init__(self, api_keys):
        self.api_keys = api_keys
        self.current_index = 0
        self.failed_keys = set()
    
    def get_current_key(self):
        if len(self.failed_keys) == len(self.api_keys):
            # æ‰€æœ‰å¯†é’¥éƒ½å¤±è´¥äº†ï¼Œé‡ç½®å¤±è´¥åˆ—è¡¨
            self.failed_keys.clear()
        
        while self.current_index in self.failed_keys:
            self.current_index = (self.current_index + 1) % len(self.api_keys)
        
        return self.api_keys[self.current_index]
    
    def mark_key_failed(self):
        self.failed_keys.add(self.current_index)
        self.current_index = (self.current_index + 1) % len(self.api_keys)

# ä½¿ç”¨ç¤ºä¾‹
api_keys = [
    "sk-key1...",
    "sk-key2...",
    "sk-key3..."
]
key_rotator = APIKeyRotator(api_keys)
```

### é—®é¢˜14ï¼šç½‘ç»œè¿æ¥è¶…æ—¶

**ç—‡çŠ¶**ï¼š
```python
TimeoutError: Request timeout after 30 seconds
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. è°ƒæ•´è¶…æ—¶è®¾ç½®
api_config = {
    "timeout": 60,          # å¢åŠ åˆ°60ç§’
    "connect_timeout": 10,  # è¿æ¥è¶…æ—¶
    "read_timeout": 50      # è¯»å–è¶…æ—¶
}

# 2. å®ç°è¿æ¥æ± 
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class APIClient:
    def __init__(self):
        self.session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=10, pool_maxsize=20)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def make_request(self, url, data, timeout=60):
        try:
            response = self.session.post(url, json=data, timeout=timeout)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.Timeout:
            print("è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            raise
        except requests.exceptions.ConnectionError:
            print("ç½‘ç»œè¿æ¥é”™è¯¯")
            raise

# 3. ç½‘ç»œè¯Šæ–­å·¥å…·
def diagnose_network():
    import socket
    import urllib.request
    
    # æ£€æŸ¥DNSè§£æ
    try:
        socket.gethostbyname('api.openai.com')
        print("âœ… DNSè§£ææ­£å¸¸")
    except socket.gaierror:
        print("âŒ DNSè§£æå¤±è´¥")
    
    # æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
    try:
        urllib.request.urlopen('https://api.openai.com', timeout=10)
        print("âœ… ç½‘ç»œè¿é€šæ€§æ­£å¸¸")
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¿é€šæ€§é—®é¢˜: {e}")
    
    # æ£€æŸ¥ä»£ç†è®¾ç½®
    proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
    if proxy:
        print(f"ğŸ” æ£€æµ‹åˆ°ä»£ç†è®¾ç½®: {proxy}")

diagnose_network()
```

## æ•°æ®åº“è¿æ¥é—®é¢˜

### é—®é¢˜15ï¼šChromaDBè¿æ¥å¤±è´¥

**ç—‡çŠ¶**ï¼š
```python
ConnectionError: Could not connect to ChromaDB
```

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
# æ£€æŸ¥ChromaDBé…ç½®
def diagnose_chromadb():
    from core.knowledge_base.models import ChromaDBConfig
    
    config = ChromaDBConfig(
        connection_type="local",
        path="./data/chromadb"
    )
    
    print(f"è¿æ¥ç±»å‹: {config.connection_type}")
    print(f"æ•°æ®è·¯å¾„: {config.path}")
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(config.path):
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {config.path}")
        os.makedirs(config.path, exist_ok=True)
        print(f"âœ… å·²åˆ›å»ºæ•°æ®è·¯å¾„: {config.path}")
    
    # æ£€æŸ¥æƒé™
    if not os.access(config.path, os.W_OK):
        print(f"âŒ æ•°æ®è·¯å¾„æ— å†™æƒé™: {config.path}")
    else:
        print(f"âœ… æ•°æ®è·¯å¾„æƒé™æ­£å¸¸")

diagnose_chromadb()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. é‡æ–°åˆå§‹åŒ–æ•°æ®åº“
def reset_chromadb():
    import shutil
    import os
    
    db_path = "./data/chromadb"
    
    if os.path.exists(db_path):
        print(f"åˆ é™¤ç°æœ‰æ•°æ®åº“: {db_path}")
        shutil.rmtree(db_path)
    
    print(f"é‡æ–°åˆ›å»ºæ•°æ®åº“ç›®å½•: {db_path}")
    os.makedirs(db_path, exist_ok=True)
    
    # é‡æ–°åˆå§‹åŒ–ç®¡ç†å™¨
    from core.knowledge_base.manager import KnowledgeBaseManager
    kb_manager = KnowledgeBaseManager()
    
    print("âœ… æ•°æ®åº“é‡æ–°åˆå§‹åŒ–å®Œæˆ")

# 2. æ•°æ®åº“å¥åº·æ£€æŸ¥
def check_chromadb_health():
    try:
        import chromadb
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = chromadb.PersistentClient(path="./data/chromadb")
        
        # åˆ—å‡ºé›†åˆ
        collections = client.list_collections()
        print(f"âœ… ChromaDBè¿æ¥æ­£å¸¸ï¼Œé›†åˆæ•°é‡: {len(collections)}")
        
        # æµ‹è¯•åˆ›å»ºå’Œåˆ é™¤é›†åˆ
        test_collection = client.create_collection("test_health_check")
        client.delete_collection("test_health_check")
        print("âœ… æ•°æ®åº“è¯»å†™æµ‹è¯•é€šè¿‡")
        
        return True
    except Exception as e:
        print(f"âŒ ChromaDBå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False

# 3. æ•°æ®åº“ä¿®å¤å·¥å…·
def repair_chromadb():
    print("å¼€å§‹ä¿®å¤ChromaDB...")
    
    # å¤‡ä»½ç°æœ‰æ•°æ®
    import shutil
    import datetime
    
    db_path = "./data/chromadb"
    backup_path = f"./data/chromadb_backup_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if os.path.exists(db_path):
        shutil.copytree(db_path, backup_path)
        print(f"âœ… æ•°æ®å·²å¤‡ä»½åˆ°: {backup_path}")
    
    # å°è¯•ä¿®å¤
    try:
        check_chromadb_health()
        print("âœ… æ•°æ®åº“ä¿®å¤æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“ä¿®å¤å¤±è´¥: {e}")
        print("å»ºè®®é‡æ–°åˆå§‹åŒ–æ•°æ®åº“")
```

## UIç•Œé¢é—®é¢˜

### é—®é¢˜16ï¼šUIç•Œé¢æ— æ³•å¯åŠ¨

**ç—‡çŠ¶**ï¼š
```python
ImportError: No module named 'PySide6'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£…UIä¾èµ–
pip install PySide6

# å¦‚æœå®‰è£…å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
conda install pyside6

# æˆ–è€…ä½¿ç”¨ç³»ç»ŸåŒ…ç®¡ç†å™¨ï¼ˆUbuntuï¼‰
sudo apt-get install python3-pyside6
```

### é—®é¢˜17ï¼šUIç•Œé¢æ˜¾ç¤ºå¼‚å¸¸

**è¯Šæ–­æ­¥éª¤**ï¼š
```python
# æ£€æŸ¥Qtç¯å¢ƒ
def check_qt_environment():
    try:
        from PySide6.QtWidgets import QApplication
        from PySide6.QtCore import QT_VERSION_STR
        
        print(f"Qtç‰ˆæœ¬: {QT_VERSION_STR}")
        
        # åˆ›å»ºåº”ç”¨ç¨‹åºå®ä¾‹
        app = QApplication.instance()
        if app is None:
            app = QApplication([])
        
        print("âœ… Qtç¯å¢ƒæ­£å¸¸")
        return True
    except Exception as e:
        print(f"âŒ Qtç¯å¢ƒé—®é¢˜: {e}")
        return False

check_qt_environment()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. è®¾ç½®Qtç¯å¢ƒå˜é‡
import os
os.environ['QT_QPA_PLATFORM'] = 'xcb'  # Linux
# os.environ['QT_QPA_PLATFORM'] = 'windows'  # Windows

# 2. å¤„ç†é«˜DPIæ˜¾ç¤º
from PySide6.QtWidgets import QApplication
from PySide6.QtCore import Qt

def setup_high_dpi():
    QApplication.setAttribute(Qt.AA_EnableHighDpiScaling, True)
    QApplication.setAttribute(Qt.AA_UseHighDpiPixmaps, True)

# 3. é”™è¯¯å¤„ç†åŒ…è£…å™¨
def safe_ui_startup():
    try:
        setup_high_dpi()
        
        app = QApplication([])
        
        from ui.knowledge_base_panel import KnowledgeBasePanel
        panel = KnowledgeBasePanel()
        panel.show()
        
        return app.exec()
    except Exception as e:
        print(f"UIå¯åŠ¨å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œç•Œé¢...")
        return False
```

## æ—¥å¿—å’Œè°ƒè¯•

### é…ç½®è¯¦ç»†æ—¥å¿—

```python
import logging
import sys
from datetime import datetime

def setup_logging(level=logging.DEBUG):
    # åˆ›å»ºæ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(formatter)
    
    # æ–‡ä»¶å¤„ç†å™¨
    log_filename = f"logs/kb_debug_{datetime.now().strftime('%Y%m%d')}.log"
    os.makedirs(os.path.dirname(log_filename), exist_ok=True)
    
    file_handler = logging.FileHandler(log_filename, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    
    # é…ç½®ç‰¹å®šæ¨¡å—çš„æ—¥å¿—çº§åˆ«
    logging.getLogger('chromadb').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    
    print(f"æ—¥å¿—å·²é…ç½®ï¼Œæ–‡ä»¶: {log_filename}")

# å¯ç”¨è¯¦ç»†æ—¥å¿—
setup_logging(logging.DEBUG)
```

### è°ƒè¯•å·¥å…·

```python
class DebugHelper:
    def __init__(self):
        self.debug_enabled = True
    
    def debug_search(self, query, results):
        if not self.debug_enabled:
            return
        
        print(f"\nğŸ” è°ƒè¯•æœç´¢: {query}")
        print(f"ç»“æœæ•°é‡: {len(results)}")
        
        for i, result in enumerate(results[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
            print(f"\nç»“æœ {i+1}:")
            print(f"  ç›¸å…³åº¦: {result.relevance_score:.4f}")
            print(f"  æ¥æº: {result.source_document}")
            print(f"  å†…å®¹: {result.content[:200]}...")
    
    def debug_processing(self, file_path, status):
        if not self.debug_enabled:
            return
        
        print(f"\nğŸ“„ è°ƒè¯•å¤„ç†: {file_path}")
        print(f"çŠ¶æ€: {status.status.value}")
        if status.error_message:
            print(f"é”™è¯¯: {status.error_message}")
    
    def debug_performance(self, operation, duration):
        if not self.debug_enabled:
            return
        
        print(f"\nâ±ï¸  æ€§èƒ½è°ƒè¯•: {operation}")
        print(f"è€—æ—¶: {duration:.3f}ç§’")
        
        if duration > 5:
            print("âš ï¸ æ“ä½œè€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–")

debug_helper = DebugHelper()

# ä½¿ç”¨è°ƒè¯•å·¥å…·
results = kb_manager.search_knowledge("æµ‹è¯•æŸ¥è¯¢", [collection_id])
debug_helper.debug_search("æµ‹è¯•æŸ¥è¯¢", results)
```

### ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š

```python
def generate_diagnostic_report():
    """ç”Ÿæˆå®Œæ•´çš„ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š"""
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "system_info": {},
        "configuration": {},
        "performance": {},
        "errors": []
    }
    
    try:
        # ç³»ç»Ÿä¿¡æ¯
        import platform
        report["system_info"] = {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024
        }
        
        # é…ç½®ä¿¡æ¯
        report["configuration"] = {
            "kb_config": get_knowledge_base_config() if get_knowledge_base_config else {},
            "chromadb_config": get_chromadb_config() if get_chromadb_config else {}
        }
        
        # æ€§èƒ½ä¿¡æ¯
        if hasattr(optimized_manager, 'get_performance_insights'):
            insights = optimized_manager.get_performance_insights()
            report["performance"] = insights
        
        # æ£€æŸ¥å„ä¸ªç»„ä»¶
        components = [
            ("ChromaDB", check_chromadb_health),
            ("APIé…ç½®", diagnose_api_config),
            ("å†…å­˜ä½¿ç”¨", check_memory_usage),
            ("ç£ç›˜ç©ºé—´", lambda: check_disk_space() > 1024*1024*1024)
        ]
        
        for name, check_func in components:
            try:
                result = check_func()
                report[f"{name}_status"] = "OK" if result else "FAILED"
            except Exception as e:
                report[f"{name}_status"] = f"ERROR: {str(e)}"
                report["errors"].append(f"{name}: {str(e)}")
    
    except Exception as e:
        report["errors"].append(f"è¯Šæ–­è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"diagnostic_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    return report

# ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
diagnostic_report = generate_diagnostic_report()
```

## è·å–å¸®åŠ©

å¦‚æœä»¥ä¸Šè§£å†³æ–¹æ¡ˆéƒ½æ— æ³•è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·ï¼š

1. **æ”¶é›†ä¿¡æ¯**ï¼š
   - é”™è¯¯çš„å®Œæ•´å †æ ˆè·Ÿè¸ª
   - ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š
   - ç›¸å…³çš„é…ç½®æ–‡ä»¶
   - é‡ç°é—®é¢˜çš„æ­¥éª¤

2. **æ£€æŸ¥æ—¥å¿—**ï¼š
   - æŸ¥çœ‹è¯¦ç»†çš„æ—¥å¿—æ–‡ä»¶
   - æ³¨æ„é”™è¯¯å‘ç”Ÿçš„æ—¶é—´å’Œä¸Šä¸‹æ–‡

3. **è”ç³»æ”¯æŒ**ï¼š
   - æä¾›è¯¦ç»†çš„é—®é¢˜æè¿°
   - åŒ…å«ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
   - é™„ä¸Šè¯Šæ–­æŠ¥å‘Š

---

*æœ¬æ•…éšœæ’é™¤æŒ‡å—æ¶µç›–äº†çŸ¥è¯†åº“RAGç³»ç»Ÿçš„å¸¸è§é—®é¢˜ã€‚å¦‚æœé‡åˆ°æœªåˆ—å‡ºçš„é—®é¢˜ï¼Œè¯·å‚è€ƒç³»ç»Ÿæ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯è¿›è¡Œè¯Šæ–­ã€‚*